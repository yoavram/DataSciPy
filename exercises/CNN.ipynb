{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN: Fashion-MNIST \n",
    "\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n",
    "\n",
    "\n",
    "| Label |\tDescription|\n",
    "|---|------------------|\n",
    "| 0 |\tT-shirt/top    |\n",
    "| 1 |\tTrouser        |\n",
    "| 2 |\tPullover       |\n",
    "| 3 |\tDress          |\n",
    "| 4 |\tCoat           |\n",
    "| 5 |\tSandal         |\n",
    "| 6 |\tShirt          |\n",
    "| 7 |\tSneaker        |\n",
    "| 8 |\tBag            |\n",
    "| 9 |\tAnkle boot     |\n",
    "\n",
    "See [keras docs](https://keras.io/datasets/).\n",
    "\n",
    "In this exercise we will train a CNN on the dataset.\n",
    "You can use either TensorFlow or Keras.\n",
    "The [solution](../solutions/CNN.ipynb) is in the `solutions` folder.\n",
    "\n",
    "We'll get the data via [`keras.datasets`](https://keras.io/datasets/).\n",
    "It takes some time to download, and you should also install keras (`conda install keras`) rather than use the keras supplied with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    import keras\n",
    "except ModuleNotFoundError:\n",
    "    from tensorflow import keras\n",
    "print('GPU:', tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the images to a float32 between 0 and 1 and reshape to 28x28x1 (only one channel for black and white) because 2D convolutions expect 3D images (3rd dimension is channel or image).\n",
    "\n",
    "We also need to one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD9CAYAAAB3NXH8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANtUlEQVR4nO3d3U/W9R/H8fcFgqLciQoKSwU1oRBQsmUznCkVtdhqa3XSQSetddQfYGtzdVRbJ/UHtFUH2d3caqnrZszhLXRHYpBimIka4h0yILx+Z7/9tt/v+3r/BAku3s/H6WufC7wuX37n3tfn80ml02kDEEfWTP8CAP5ZlB4IhtIDwVB6IBhKDwRD6YFgKD0QDKUHgqH0QDCUHgiG0gPBUHogmHl3+gVTqVSfmRWa2Zk7/doA/m21mV1Lp9OVt7vwjpfezArz8vJKampqSqbhtUO7dOlSYjY+Pi7X5ufny/zmzZsyHxsbk/mKFSsSs5ycHLkWt6+7u9tGRkYmtXY6Sn+mpqampKOjYxpeevqprcapVOof/E3+2zvvvJOYDQwMyLVbt26V+fHjx2V+7tw5me/atSsxq6iokGtx+xobG62zs/PMZNbyf3ogGEoPBEPpgWAoPRAMpQeCofRAMNMxspvVvNN/p3Msp+bsZmavvfaazNeuXZuYDQ8Py7WvvPKKzNWc3cysqalJ5s3NzZP+2S+++KLMcWfxpAeCofRAMJQeCIbSA8FQeiAYSg8EQ+mBYObcnH6qc3i1r/z999+Xa/fs2SPz6upqmS9btkzmvb29iVlDQ4Ncu3fvXplfvnxZ5hMTEzJXs/iTJ0/KtS0tLTJ/7LHHZF5XV5eYbd++Xa6NiCc9EAylB4Kh9EAwlB4IhtIDwVB6IJg5N7Kb6tbY3bt3J2be9tOnn35a5vPnz5d5aWmpzIuKihIzb+zl5d5I7o033pD5d999N+nXfv7552X++eefy3zfvn2JWVaWfq5t27ZN5jO5FXu68KQHgqH0QDCUHgiG0gPBUHogGEoPBEPpgWAybk4/1bnpwYMHZa5uWF28eLFcu3TpUpn//fffMvdunh0dHU3M3nzzzSn9bO99e/fdd2V+5MiRxCw7O1uu9W44XrduncyLi4sTsw8++ECu3bx5s8wXLlwo80yc4/OkB4Kh9EAwlB4IhtIDwVB6IBhKDwRD6YFgMm5OP9W5540bN2R+4cKFxEztZzcz6+zslPkjjzwi84sXL8p8bGwsMXvooYfkWu94bvUdADOzEydOyLyrqysx847nPnXqlMyvXLkic/Udgfvuu29KP3vDhg0yn41zeA9PeiAYSg8EQ+mBYCg9EAylB4Kh9EAwlB4IJuPm9FO1f/9+mauZsjdHX7Bggczb2tpknpubK3O1r/yTTz6Ra5cvXy7zoaEhmbe3t8tczbO9a7Dz8vJk7p1dX1tbm5h5n9mPP/4oc29On4l40gPBUHogGEoPBEPpgWAoPRAMpQeCmXMju5s3b8q8qqpK5oWFhYnZn3/+Kdd6YzEv947A7uvrS8xWrVol16ptuWb+WOzhhx+WuaK23ZqZrVmzRuZnz56VuTp6vLy8XK7NycmR+VzEkx4IhtIDwVB6IBhKDwRD6YFgKD0QDKUHgplzc3pv7trf3z/p9V9//bVcOzExIfPdu3fL3Julq3l2QUGBXLt3716Ze9dse1tQ1RbW1tZWudbbevvWW2/J/Nlnn5W5cvz48Wl77dmKJz0QDKUHgqH0QDCUHgiG0gPBUHogGEoPBDPn5vTelcrenna13/706dNybX5+vsyXLFkic29f+ZdffpmYbdy4Ua71vr/gHUO9aNEimTc1NSVm3hzeO0cgnU7LXF0vXllZKdfu2LFD5t7vXlJSIvPZiCc9EAylB4Kh9EAwlB4IhtIDwVB6IBhKDwQz5+b03hnp2dnZMj927FhitnDhQrnWm9l6V1V7s3C1t/vq1aty7alTp2Te09Mjc4/63X///Xe5duvWrTJfvXq1zK9fv56Y/fLLL3KtmvGbmbW0tMg8E/GkB4Kh9EAwlB4IhtIDwVB6IBhKDwRD6YFg5tycXs1szfy57B9//JGY5ebmyrVeXl9fL/O//vpL5iMjI4lZe3u7XFtdXS3zvr4+mTc0NMhczdKHhobkWu+7FR71vj3zzDNy7blz52Tufb9hw4YNMp+NeNIDwVB6IBhKDwRD6YFgKD0QDKUHgplzIztvG2dZWZnMe3t7E7Ph4WG51tsaOz4+LvPR0VGZX7lyJTF74okn5Nr9+/fL3Bv5FRUVyVxt7fWOHfeOLfc+s6NHjyZm3pZh79hx731hZAdg1qP0QDCUHgiG0gPBUHogGEoPBEPpgWDm3Jy+vLxc5gcOHJC52r6alaX/jVyxYoXMu7q6ZL5q1SqZFxYWJmaHDh2Sa9Wfy8xs586dMve25qrvMHjfb/D+3Ldu3ZK5ukK8v79fri0oKJB5Js7hPTzpgWAoPRAMpQeCofRAMJQeCIbSA8FQeiCYOTen945yHhsbk7naj59Op+Va76rqu+++W+berH3btm2Jmbff3Xtfbty4IXPvmGp13XRpaalc++uvv8rc2/P+2WefJWbLli2Ta7335fLlyzJvbm6W+WzEkx4IhtIDwVB6IBhKDwRD6YFgKD0QDKUHgsm4Ob06+93Mn+l6+6vVVdeVlZVy7ZYtW2R+/vx5mXvXQQ8MDCRm3n75uro6mXtz+LVr18pczcO91+7u7pZ5Y2OjzNXv5n3eVVVVMi8uLpZ5JuJJDwRD6YFgKD0QDKUHgqH0QDCUHgiG0gPBZNyc3ptHZ2dny9zbN65e3zu/Xe3FN/P30//2228yv//++xMz70z+I0eOyHx0dFTm9fX1Mr9w4UJi5p0zoPbim5nl5OTIXJ1N39bWJtd6n6m319/bb+/92WcCT3ogGEoPBEPpgWAoPRAMpQeCofRAMBk3slPbS83MBgcHZe5di7xgwYLEzNs6u3HjRpkvXbpU5t7YrbOzMzFbt26dXHvPPffI3DsK+uLFizJXo1DvfTl27JjM1Wdiprfuzpun/4qnUimZt7S0yLynp0fmDzzwgMxnAk96IBhKDwRD6YFgKD0QDKUHgqH0QDCUHggm4+b03tbaoaEhmefm5spczXy9mat3PPfhw4dlvn37dpmr47nHx8fl2n379sncO2ba+37E5s2bE7OpHs/tbV8tKChIzK5evSrXnjlzRube35edO3fKfDbiSQ8EQ+mBYCg9EAylB4Kh9EAwlB4IhtIDwWTcnN7b162OiTYze++992R+7733JmbLly+Xa715snfU8xdffCFztbdbHUFtZtbc3Czzjz/+WObr16+XuTr+25uFe98R8FRUVCRm6XRarlVXbJuZlZaWyvyHH36Q+cqVK2U+E3jSA8FQeiAYSg8EQ+mBYCg9EAylB4Kh9EAwGTen9+bRhYWFMi8rK5P56dOnE7MHH3xQrn311VdlXltbK3O1J91MXyftnR1fVVUl80cffVTmS5YskXlxcXFi5s3C+/v7Ze7taZ8/f35i1tvbK9d6dxE89dRTMj948KDMW1tbZT4TeNIDwVB6IBhKDwRD6YFgKD0QDKUHgsm4kd2iRYtk/vPPP8tcXfdsZvbCCy8kZgcOHJBrvd/NG5t5v7vKX3rpJbn2o48+kvlXX30l8yeffFLmJ0+eTMy8Lcne9eHemPbEiROJ2a5du+Ta119/XebeEdr19fUyn4140gPBUHogGEoPBEPpgWAoPRAMpQeCofRAMBk3p/euovaOwD506JDMU6lUYuZdqTw8PCxzb87f0NAgc7V99cMPP5Rr583TH/Vzzz0nc29rreJd4f3999/LXF1Fbaa3x3o/29u2683pvSPZZyOe9EAwlB4IhtIDwVB6IBhKDwRD6YFgKD0QTMbN6W/duiVz77po7+rhs2fPJmYdHR1ybXt7u8zz8vJkvmfPHpmrI7izsvS/3+r4bDP/z5aTkyPza9euJWbeFd3eMdTnz5+X+V133ZWYqeu9zczefvttmXvfT/DOCpiNeNIDwVB6IBhKDwRD6YFgKD0QDKUHgqH0QDAZN6dfs2aNzHt6emSenZ0tc3V+/MDAgFzrUVcqm5mtX79e5ur8d2/feGNjo8zLy8tlfunSJZmrz8W7ztnb6+99v0G9r97V5d73D9T3NszMampqZD4b8aQHgqH0QDCUHgiG0gPBUHogGEoPBEPpgWAybk4/MjIi87GxMZkPDg7KvKioKDHz9qx782RvT7uXq5nz9evX5dqjR4/K/PDhwzL3PP7444mZOq///8k9W7ZsmfTaxYsXy7ytrU3mJSUlk/7ZM4UnPRAMpQeCofRAMJQeCIbSA8FQeiCYjBvZeaMp70jin376SebqCO2VK1fKtRUVFTKfmJiQeV9fn8ybmpoSM+/KZW+UqY7XNjPLz8+XeVdXV2LmHUvufWbeKLSsrEzmirf19uWXX5Z5dXX1pH/2TOFJDwRD6YFgKD0QDKUHgqH0QDCUHgiG0gPBpNLp9J19wVSqY9OmTZu8q48j+vbbb2X+zTffyLyuri4xq62tlWu97wh4c3hvVv7pp58mZt6W4dbWVplXVVXJPKLGxkbr7OzsTKfT+mzz/4EnPRAMpQeCofRAMJQeCIbSA8FQeiCY6RjZDebl5ZVk4m2e083bFuzlamzmjdS8z9m7zdc7CXhoaGjSP1udQGzm3/YbUXd3t42MjFxOp9NLbnftdJS+z8wKzezMHX1hAP9ptZldS6fTlbe78I6XHsDsxv/pgWAoPRAMpQeCofRAMJQeCIbSA8FQeiAYSg8EQ+mBYCg9EAylB4Kh9EAwlB4IhtIDwVB6IBhKDwRD6YFgKD0QDKUHgvkXOmqEltpsXigAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 126,
       "width": 126
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, x_train.shape[0])\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_train[i, :, :, 0], cmap='gray_r')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "print(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build and train the CNN.\n",
    "When I trained a CNN I got this accuracy of ~87% on the test set, see if you can top it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
