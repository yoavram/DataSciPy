{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN: Fashion-MNIST \n",
    "\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n",
    "\n",
    "\n",
    "| Label |\tDescription|\n",
    "|---|------------------|\n",
    "| 0 |\tT-shirt/top    |\n",
    "| 1 |\tTrouser        |\n",
    "| 2 |\tPullover       |\n",
    "| 3 |\tDress          |\n",
    "| 4 |\tCoat           |\n",
    "| 5 |\tSandal         |\n",
    "| 6 |\tShirt          |\n",
    "| 7 |\tSneaker        |\n",
    "| 8 |\tBag            |\n",
    "| 9 |\tAnkle boot     |\n",
    "\n",
    "See [keras docs](https://keras.io/datasets/).\n",
    "\n",
    "In this exercise we will train a CNN on the dataset.\n",
    "You can use either TensorFlow or Keras.\n",
    "\n",
    "We'll get the data via [`keras.datasets`](https://keras.io/datasets/).\n",
    "It takes some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    import keras\n",
    "except ModuleNotFoundError:\n",
    "    from tensorflow import keras\n",
    "print('Keras', keras.__version__)\n",
    "print('GPU:', tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the images to a float32 between 0 and 1 and reshape to 28x28x1 (only one channel for black and white) because 2D convolutions expect 3D images (3rd dimension is channel or image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 126,
       "width": 126
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, x_train.shape[0])\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_train[i, :, :, 0], cmap='gray_r')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "print(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build and train the CNN.\n",
    "When I trained a CNN I got this accuracy of ~87% on the test set, see if you can top it.\n",
    "\n",
    "We also need to one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3, 3)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(num_classes))\n",
    "model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.9226 - accuracy: 0.6696 - val_loss: 0.5855 - val_accuracy: 0.7789\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.5837 - accuracy: 0.7840 - val_loss: 0.5088 - val_accuracy: 0.8150\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.5150 - accuracy: 0.8107 - val_loss: 0.4531 - val_accuracy: 0.8389\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4682 - accuracy: 0.8308 - val_loss: 0.4225 - val_accuracy: 0.8496\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4386 - accuracy: 0.8421 - val_loss: 0.4036 - val_accuracy: 0.8558\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4172 - accuracy: 0.8500 - val_loss: 0.3824 - val_accuracy: 0.8636\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.3991 - accuracy: 0.8568 - val_loss: 0.3709 - val_accuracy: 0.8653\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.3853 - accuracy: 0.8629 - val_loss: 0.3550 - val_accuracy: 0.8727\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.3704 - accuracy: 0.8676 - val_loss: 0.3492 - val_accuracy: 0.8728\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.3618 - accuracy: 0.8695 - val_loss: 0.3378 - val_accuracy: 0.8779\n"
     ]
    }
   ],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=50,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test, y_test)\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.33782794494628904\n",
      "Test accuracy: 0.8779\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/keras_cnn_fashion_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
