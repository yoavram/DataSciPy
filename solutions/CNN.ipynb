{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN: Fashion-MNIST \n",
    "\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n",
    "\n",
    "\n",
    "| Label |\tDescription|\n",
    "|---|------------------|\n",
    "| 0 |\tT-shirt/top    |\n",
    "| 1 |\tTrouser        |\n",
    "| 2 |\tPullover       |\n",
    "| 3 |\tDress          |\n",
    "| 4 |\tCoat           |\n",
    "| 5 |\tSandal         |\n",
    "| 6 |\tShirt          |\n",
    "| 7 |\tSneaker        |\n",
    "| 8 |\tBag            |\n",
    "| 9 |\tAnkle boot     |\n",
    "\n",
    "See [keras docs](https://keras.io/datasets/).\n",
    "\n",
    "In this exercise we will train a CNN on the dataset.\n",
    "You can use either TensorFlow or Keras.\n",
    "\n",
    "We'll get the data via [`keras.datasets`](https://keras.io/datasets/).\n",
    "It takes some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.13.1\n",
      "Keras: 2.13.1\n",
      "GPU: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print('Tensorflow:', tf.__version__)    \n",
    "print('Keras:', keras.__version__)\n",
    "print('GPU:', tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the images to a float32 between 0 and 1 and reshape to 28x28x1 (only one channel for black and white) because 2D convolutions expect 3D images (3rd dimension is channel or image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"125.28pt\" height=\"125.28pt\" viewBox=\"0 0 125.28 125.28\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-11-08T22:39:19.285684</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 125.28 \n",
       "L 125.28 125.28 \n",
       "L 125.28 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 7.2 118.08 \n",
       "L 118.08 118.08 \n",
       "L 118.08 7.2 \n",
       "L 7.2 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p75c10c6bfa)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image61acbf27a1\" transform=\"scale(1 -1) translate(0 -110.88)\" x=\"7.2\" y=\"-7.2\" width=\"110.88\" height=\"110.88\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\"/>\n",
       "   <g id=\"matplotlib.axis_2\"/>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 7.2 118.08 \n",
       "L 7.2 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 118.08 118.08 \n",
       "L 118.08 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 7.2 118.08 \n",
       "L 118.08 118.08 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 7.2 7.2 \n",
       "L 118.08 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p75c10c6bfa\">\n",
       "   <rect x=\"7.2\" y=\"7.2\" width=\"110.88\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 174,
       "width": 174
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, x_train.shape[0])\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_train[i, :, :, 0], cmap='gray_r')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "print(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build and train the CNN.\n",
    "When I trained a CNN I got this accuracy of ~87% on the test set, see if you can top it.\n",
    "\n",
    "We also need to one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3, 3)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(num_classes))\n",
    "model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1200/1200 [==============================] - 50s 40ms/step - loss: 0.5534 - accuracy: 0.7980 - val_loss: 0.3942 - val_accuracy: 0.8517\n",
      "Epoch 2/10\n",
      "1200/1200 [==============================] - 60s 50ms/step - loss: 0.3678 - accuracy: 0.8654 - val_loss: 0.3154 - val_accuracy: 0.8840\n",
      "Epoch 3/10\n",
      "1200/1200 [==============================] - 60s 50ms/step - loss: 0.3200 - accuracy: 0.8816 - val_loss: 0.2868 - val_accuracy: 0.8933\n",
      "Epoch 4/10\n",
      "1200/1200 [==============================] - 64s 53ms/step - loss: 0.2911 - accuracy: 0.8924 - val_loss: 0.2761 - val_accuracy: 0.8969\n",
      "Epoch 5/10\n",
      "1200/1200 [==============================] - 60s 50ms/step - loss: 0.2694 - accuracy: 0.9007 - val_loss: 0.2618 - val_accuracy: 0.9022\n",
      "Epoch 6/10\n",
      "1200/1200 [==============================] - 53s 45ms/step - loss: 0.2566 - accuracy: 0.9047 - val_loss: 0.2530 - val_accuracy: 0.9038\n",
      "Epoch 7/10\n",
      "1200/1200 [==============================] - 51s 43ms/step - loss: 0.2425 - accuracy: 0.9097 - val_loss: 0.2485 - val_accuracy: 0.9084\n",
      "Epoch 8/10\n",
      "1200/1200 [==============================] - 65s 54ms/step - loss: 0.2356 - accuracy: 0.9118 - val_loss: 0.2547 - val_accuracy: 0.9038\n",
      "Epoch 9/10\n",
      "1200/1200 [==============================] - 72s 60ms/step - loss: 0.2238 - accuracy: 0.9159 - val_loss: 0.2421 - val_accuracy: 0.9100\n",
      "Epoch 10/10\n",
      "1200/1200 [==============================] - 776s 647ms/step - loss: 0.2136 - accuracy: 0.9195 - val_loss: 0.2426 - val_accuracy: 0.9107\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=50,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test, y_test)\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.24262717366218567\n",
      "Test accuracy: 0.9107000231742859\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('../data/keras_cnn_fashion_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
