{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](../logo.png)\n",
    "\n",
    "# Gated  Recurrent Unit\n",
    "## Yoav Ram\n",
    "\n",
    "In this session we will expand over RNN with GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax 0.4.30 gpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax \n",
    "import jax.numpy as np\n",
    "print('jax', jax.__version__, jax.default_backend())\n",
    "import numpy as onp\n",
    "import optax # pip install optax\n",
    "\n",
    "from collections import Counter\n",
    "from random import uniform\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data is just text data, in this case Shakespear's writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 99993\n",
      "Number of unique characters: 62\n",
      "Number of lines: 3298\n",
      "Number of words: 15893\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "That, poor contempt, or claim'd thou slept so faithful,\n",
      "I may contrive our father; and, in their defeated queen,\n",
      "Her flesh broke me and puttance of expedition house,\n",
      "And in that same that ever I lament this stomach,\n",
      "And he, nor Butly and my fury, knowing everything\n",
      "Grew daily ever, his great strength and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, that it may not pray their patience.'\n",
      "\n",
      "KING LEAR:\n",
      "The instant common maid, as we may less be\n",
      "a brave gentleman and joiner: he that finds u\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/shakespear.txt'\n",
    "with open(filename, 'rt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating \n",
    "- a list `chars` of the unique characters\n",
    "- `data_size` the number of total characters\n",
    "- `vocab_size` the number of unique characters\n",
    "- `int_to_char` a dictionary from index to char\n",
    "- `char_to_int` a dictionary from char to index\n",
    "and then we convert `data` from a string to a NumPy array of integers representing the chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "\n",
    "# char to int and vice versa\n",
    "int_to_char = dict(enumerate(chars)) #  == { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_int = dict(zip(int_to_char.values(), int_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "\n",
    "def onehot_encode(text):\n",
    "    ints = [char_to_int[c] for c in text]\n",
    "    ints = np.array(ints, dtype=int)\n",
    "    return jax.nn.one_hot(ints, vocab_size)\n",
    "\n",
    "def onehot_decode(data):\n",
    "    ints = data.argmax(axis=1).tolist()\n",
    "    chars = (int_to_char[k] for k in ints)\n",
    "    return str.join('', chars)\n",
    "\n",
    "X = onehot_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model\n",
    "\n",
    "The GRU extends RNN. It avoids the vanishing gradient problem for the vanilla RNN, and is more efficient than LSTM (long-short time memory).\n",
    "To compute the update to the hidden memory layer $h_t$, it first computes a _reset gate_ $r_t$ and an update gate $z_t$ that are used to interpoltate between the candidate memory $\\tilde h_t$ and the next $h_t$.\n",
    "\n",
    "- $x_t$ is the $t$ character, one-hot encoded and a 1D array of length `vocab_size`.\n",
    "- $h_t$ is the state of the hidden memory layer after seeing $t$ characters, encoded as a 1D array of numbers (neurons...)\n",
    "- $r_t$ is the _reset gate_\n",
    "- $z_t$ is the _update gate_\n",
    "- $\\tilde h_t$ is the candidate hidden memory\n",
    "- $\\widehat y_t$ is the prediction of the network after seeing $t$ characters, encoded as a 1D array of probabilities of length `vocab_size`\n",
    "- $\\sigma(x)$ is the sigmoid/logistic function\n",
    "- $\\circ$ is the Hadamard or element-wise product, $x \\circ y = (x_1 y_1, \\ldots x_n y_n)$.\n",
    "\n",
    "The model is then written as:\n",
    "$$\n",
    "z_t = \\sigma{\\left(W_x^z x_t + W_h^z h_{t-1} + b_z\\right)}\n",
    "$$\n",
    "$$\n",
    "r_t = \\sigma{\\left(W_x^r x_t + W_h^r h_{t-1} + b_r\\right)}\n",
    "$$\n",
    "$$\n",
    "\\tilde h_t = \\tanh{\\left(W_x^h x_t + W_h^h (r_t \\circ h_{t-1}) + b_h\\right)}\n",
    "$$\n",
    "$$\n",
    "h_t = (1-z_t) \\circ h_{t-1} + z_t \\circ \\tilde h_t\n",
    "$$\n",
    "$$\n",
    "\\hat y_t = \\mathrm{softmax}\\left(W_h^y h_t + b_y\\right)\n",
    "$$\n",
    "$$\n",
    "x_{t+1} \\sim \\mathrm{Cat}(\\hat{y}_t)\n",
    "$$\n",
    "\n",
    "and we set $h_0 = (0, \\ldots, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will be performed by our `step` function.\n",
    "\n",
    "## Multi-layer GRU\n",
    "We are going to layer multiple GRUs, so that the output of the first is the input of the second etc. This is done by the `layered_step` function.\n",
    "\n",
    "The `feed_forward` function will loop over a sequence of $x=(x_1, x_2, \\ldots, x_k)$ of some arbitray size - similar to batches in the FFN and CNN frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(params, x, h):\n",
    "    Wxz, Whz, Wxr, Whr, Wxh, Whh, Why, bz, br, bh, by = params\n",
    "    z = jax.nn.sigmoid(Wxz @ x + Whz @ h + bz)\n",
    "    r = jax.nn.sigmoid(Wxr @ x + Whr @ h + br)\n",
    "    tildeh = jax.nn.tanh(Wxh @ x + Whh @ (r * h) + bh)\n",
    "    h = (1 - z) * h + z * tildeh\n",
    "    yhat = jax.nn.softmax(Why @ h + by)\n",
    "    return yhat, h\n",
    "\n",
    "def layered_step(params, x, h):\n",
    "    for i in range(len(params)):\n",
    "        x, h[i] = step(params[i], x, h[i])\n",
    "    return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(params, x, h):\n",
    "    yhat = np.zeros_like(x)\n",
    "    \n",
    "    for t in range(len(x)):\n",
    "        yhat_t, h = layered_step(params, x[t], h)        \n",
    "        yhat = yhat.at[t, :].set(yhat_t) # equivalent to NumPy's yhat[t, :] = yhat_t\n",
    "\n",
    "    return yhat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(params, x, y, h):\n",
    "    yhat, h = feed_forward(params, x, h)    \n",
    "    loss = -(y * np.log(yhat)).sum()\n",
    "    return loss, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the network parameters so we can test `feed_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(key):\n",
    "    subkeys = jax.random.split(key, 7)\n",
    "    Wxr = jax.random.normal(subkeys[0], (h_size, vocab_size)) * 0.01 \n",
    "    Whr = jax.random.normal(subkeys[1], (h_size, h_size)) * 0.01\n",
    "    Wxz = jax.random.normal(subkeys[2], (h_size, vocab_size)) * 0.01 \n",
    "    Whz = jax.random.normal(subkeys[3], (h_size, h_size)) * 0.01    \n",
    "    Wxh = jax.random.normal(subkeys[4], (h_size, vocab_size)) * 0.01 \n",
    "    Whh = jax.random.normal(subkeys[5], (h_size, h_size)) * 0.01\n",
    "    Why = jax.random.normal(subkeys[6], (vocab_size, h_size)) * 0.01 \n",
    "    bz = np.zeros(h_size,) \n",
    "    br = np.zeros(h_size,) \n",
    "    bh = np.zeros(h_size,) \n",
    "    by = np.zeros(vocab_size) \n",
    "    params = Wxz, Whz, Wxr, Whr, Wxh, Whh, Why, bz, br, bh, by\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 ms ± 1.38 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "ooooooooooooooooooooooooo\n",
      "hat, poor contempt, or cl\n",
      "103.17666\n"
     ]
    }
   ],
   "source": [
    "h_size = 100 # number of units in hidden layer\n",
    "nlayers = 3\n",
    "key = jax.random.key(412) # generate new key based on the seed \"42\"\n",
    "\n",
    "init_keys = jax.random.split(key, nlayers) # one key per layer\n",
    "params = [init_params(k) for k in init_keys] # init params per layer\n",
    "h = [np.zeros(h_size) for _ in range(len(params))] # init hidden vector per layer\n",
    "\n",
    "x, y = X[:25], X[1:26]\n",
    "%timeit yhat, _ = feed_forward(params, x, h)\n",
    "yhat, _ = feed_forward(params, x, h)\n",
    "print(onehot_decode(yhat))\n",
    "print(onehot_decode(y))\n",
    "loss, h = NLL(params, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation by automatic differentiation\n",
    "\n",
    "This works in the same way as it did with RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop = jax.value_and_grad(NLL, has_aux=True)\n",
    "\n",
    "(loss, h), grads = backprop(params, x, y, h)\n",
    "for params_i, grads_i in zip(params, grads): # loop over layers\n",
    "    for p, g in zip(params_i, grads_i): # loop over params of layer\n",
    "        assert p.shape == g.shape\n",
    "        assert not (g == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer with Optax\n",
    "\n",
    "We can use a JAX implementation of the Adam optimizer from the [Optax](https://optax.readthedocs.io/) library.\n",
    "We first create the optimizer and initialize its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.001) # 0.001 is the default from Kingma et al 2014\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the optimizer to compute the updates, and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, h), grads = backprop(params, x, y, h)\n",
    "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "params = optax.apply_updates(params, updates) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JITing the training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that does all this, and pass it to `jax.jit`, which [just-in-time compiles the function](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) so it can be executed efficiently in XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit \n",
    "def update_params(params, opt_state, x, y, h):\n",
    "    (loss, h), grads = backprop(params, x, y, h)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, h, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695 µs ± 145 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit update_params(params, opt_state, x, y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.119\n",
      "103.05258\n"
     ]
    }
   ],
   "source": [
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)\n",
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the network\n",
    "\n",
    "Finally, instead of a `predict` function, we have a `sample` function, which, given the parameters and the number of samples we want, produces a sample of text from the network.\n",
    "\n",
    "It does so by drawing a random seed for $x_0$ and drawing $x_t$ for $t>0$ from the distribution given by $\\widehat y_t$.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Aven-Zhou/publication/337006979/figure/fig3/AS:821430174380045@1572855623911/An-Illustration-of-the-Generating-Sequence-in-an-RNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MjnrjaYnG,bAPAUqLR Y.RIFUsTnSfkcz\n",
      "lV;C,hHJ: obR-SyEQpGheDZze.CUmlrK,ow;ZAcL.:kS.frRpHcdmKL-H:k\n",
      "VWME'\n"
     ]
    }
   ],
   "source": [
    "def sample(params, num_samples, key):\n",
    "    h = [np.zeros(h_size) for _ in range(len(params))]\n",
    "    \n",
    "    x = np.zeros((num_samples, vocab_size), dtype=float)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    seed_char = jax.random.choice(subkey, vocab_size)\n",
    "    x = x.at[0, seed_char].set(1)\n",
    "    \n",
    "    for t in range(1, num_samples):\n",
    "        yhat, h = layered_step(params, x[t-1], h)\n",
    "        # draw from output distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        i = jax.random.choice(subkey, vocab_size, p=yhat)\n",
    "        x = x.at[t, i].set(1)\n",
    "    return onehot_decode(x)\n",
    "\n",
    "print(sample(params, 100, jax.random.key(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We setup the training - the sequence length to unroll the network, the number of batches, parameter initialization, Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "max_batches = 10000000\n",
    "h = np.zeros(h_size)\n",
    "pos = 0\n",
    "batch = 0 \n",
    "losses = []\n",
    "key = jax.random.key(28)\n",
    "key, *init_keys = jax.random.split(key)\n",
    "params = [init_params(k) for k in init_keys]\n",
    "h = [np.zeros(h_size) for _ in range(len(params))]\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.001) # you can try with 0.01\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the GRU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, loss 103.179733, pos 25\n",
      "\n",
      "hApMNxL:jTeJmQ!oXYxlJIfZeD.Lcleqz\n",
      "D.QP'fAEEmBjEgmJaCtimu:GfgsFIvLyHQjmDWM'ozUGkd? ;k!H\n",
      "bQQ:cyRVvECO,dzIls\n",
      "JWUobZpCuIykaL:jdsn-ky?lkem'KUMKPW'MrkUILiMmrlVRG zOy-BftpZ'mrK'WCaoGUe.jTdu:Lw,T?KdneD.gg.af:\n",
      "--------------------------------------------------------------------------------\n",
      "batch 1000000, loss 26.493454, pos 6275\n",
      "\n",
      "publet:\n",
      "I'll see them, True, I am boy.\n",
      "\n",
      "SfiLd:\n",
      "I holl our lood but a worth to their marriegh lenscury Matcius. Not his love.\n",
      "\n",
      "STEUS:\n",
      "More done be dot longer without 'em if from it with kous?\n",
      "\n",
      "AlTORAR:\n",
      "--------------------------------------------------------------------------------\n",
      "batch 2000000, loss 33.737244, pos 12525\n",
      "\n",
      "R-Our all Philose nobles;\n",
      "And be a reclain the so but go the prayer: and says Buthoth,\n",
      "When becontly with her wormand.\n",
      "\n",
      "OCTAVIUS Cages, but creems. Pildes of report had do you and the very fields asug\n",
      "--------------------------------------------------------------------------------\n",
      "batch 3000000, loss 15.674427, pos 18775\n",
      "\n",
      "Thinkends,\n",
      "He a borned be follow, 'ton a bounts; and it is so.\n",
      "\n",
      "TIMON:\n",
      "Shall wear exposed:\n",
      "How were his eye name,\n",
      "It see her not says my ingent, thou shilled in the earth, sir, go in piege thin;\n",
      "And d\n",
      "--------------------------------------------------------------------------------\n",
      "batch 4000000, loss 26.085833, pos 25025\n",
      "\n",
      ":\n",
      "Fier her disteries from te corond.\n",
      "\n",
      "OCTAVT:\n",
      "\n",
      "AJAX:\n",
      "Fair,\n",
      "Too an honour to thee.\n",
      "\n",
      "IAGHENENIUS:\n",
      "He shall his port. Philopopsbusion, there\n",
      "Him call'd Huge his ridection charge,, will you every one agai\n",
      "--------------------------------------------------------------------------------\n",
      "batch 5000000, loss 24.633266, pos 31275\n",
      "\n",
      "out of sweed thee,\n",
      "And no may they haire to him.\n",
      "\n",
      "CORIOLANUS:\n",
      "And by thy worthy Fraings your time shall not she duarst;\n",
      "And many think the Gightlow towers wreach and make me be;\n",
      "And father now who the\n",
      "--------------------------------------------------------------------------------\n",
      "batch 6000000, loss 23.360464, pos 37525\n",
      "\n",
      "City,\n",
      "Talewation and withtestierable not Jobungest powar's hand!\n",
      "Butly ax the ardJqury the longer.\n",
      "\n",
      "PWearly.\n",
      "\n",
      "OTHER:\n",
      "Threat a gramn'd of your aind as the fellow:\n",
      "And hear these kingdom ast was bound l\n",
      "--------------------------------------------------------------------------------\n",
      "batch 7000000, loss 26.947865, pos 43775\n",
      "\n",
      "Dus herp\n",
      "But her be cause clope you the causes outlus tale\n",
      "With the brain, your gracious ap thou to-nither, let must bagmel to the way do I know my heart,\n",
      "But so doubtled, priliain your liberty in the\n",
      "--------------------------------------------------------------------------------\n",
      "batch 8000000, loss nan, pos 50025\n",
      "\n",
      "BGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
      "--------------------------------------------------------------------------------\n",
      "batch 9000000, loss nan, pos 56275\n",
      "\n",
      "sGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
      "--------------------------------------------------------------------------------\n",
      "batch 10000000, loss nan, pos 62525\n",
      "\n",
      "jGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
      "--------------------------------------------------------------------------------\n",
      "CPU times: user 2h 57min 39s, sys: 35min 22s, total: 3h 33min 1s\n",
      "Wall time: 2h 30min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while batch <= max_batches:\n",
    "    if pos + seq_length + 1 >= data_size:\n",
    "        # reset data position and hidden state\n",
    "        pos, h = 0, [np.zeros(h_size) for _ in range(len(params))]\n",
    "        \n",
    "    x = X[pos : pos + seq_length]\n",
    "    y = X[pos + 1 : pos + seq_length + 1]\n",
    "    pos += seq_length\n",
    "        \n",
    "    params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if batch % (max_batches // 10) == 0:\n",
    "        print('batch {:d}, loss {:.6f}, pos {}'.format(batch, loss, pos))\n",
    "        print()\n",
    "        \n",
    "        with open(\"../data/gru3-jax-params-{}.pkl\".format(batch), 'wb') as file:\n",
    "            pickle.dump(params, file)\n",
    "        \n",
    "        key, subkey = jax.random.split(key)        \n",
    "        sample_text = sample(params, 200, subkey)\n",
    "        print(sample_text)\n",
    "        print('-'*80)\n",
    "    batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load parameters from specific batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 6000000\n",
    "with open(\"../data/gru3-jax-params-{}.pkl\".format(batch), 'rb') as file:\n",
    "    params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man's\n",
      "Montles, and I frower, if you wouldsple the mind of all let him to-mnom.\n",
      "\n",
      "NORFON:\n",
      "Then be degided the bruble every issemboos o'\n",
      "rag,\n",
      "Not came of our age.\n",
      "\n",
      "Hostest withse.\n",
      "\n",
      "BERTRY:\n",
      "As erame without my lives of sister betwent your face\n",
      "To-night this your foot a furge of your mourer sol\n",
      "Did amen my delised we do.\n",
      "\n",
      "ANGELO:\n",
      "Sir any fair suind;\n",
      "For: the sliem. This thren charge in grace I saw the of most heels.\n",
      "\n",
      "BOTTOF,\n",
      "Rement things in know?\n",
      "\n",
      "IAGO:\n",
      "A pleasure and even me nobly right too and lea\n"
     ]
    }
   ],
   "source": [
    "print(sample(params, 500, jax.random.key(12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blogpost\n",
    "- Cho et al. 2014. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078). arXiv:1406.1078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com).\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
