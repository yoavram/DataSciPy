{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](img/logo.png)\n",
    "\n",
    "# Recurrent Neural Networks\n",
    "## Yoav Ram\n",
    "\n",
    "In this session we will understand:\n",
    "- what recurrent neural network and how they work, and\n",
    "- how memory and state can be implemented in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing this RNN we will follow [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/)'s [blogpost about RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness) ([original code gist](https://gist.github.com/karpathy/d4dee566867f8291f086) with BSD License).\n",
    "\n",
    "The data is just text data, in this case Shakespear's writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 99993\n",
      "Number of unique characters: 62\n",
      "Number of lines: 3298\n",
      "Number of words: 15893\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "That, poor contempt, or claim'd thou slept so faithful,\n",
      "I may contrive our father; and, in their defeated queen,\n",
      "Her flesh broke me and puttance of expedition house,\n",
      "And in that same that ever I lament this stomach,\n",
      "And he, nor Butly and my fury, knowing everything\n",
      "Grew daily ever, his great strength and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, that it may not pray their patience.'\n",
      "\n",
      "KING LEAR:\n",
      "The instant common maid, as we may less be\n",
      "a brave gentleman and joiner: he that finds u\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/shakespear.txt'\n",
    "text = open(filename, 'rt').read()\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations\n",
    "\n",
    "We start by creating \n",
    "- a list `chars` of the unique characters\n",
    "- `data_size` the number of total characters\n",
    "- `vocab_size` the number of unique characters\n",
    "- `idx_to_char` a dictionary from index to char\n",
    "- `char_to_idx` a dictionary from char to index\n",
    "and then we convert `data` from a string to a NumPy array integers representing the chars via their indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "\n",
    "idx_to_char = dict(enumerate(chars)) # { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_idx = dict(zip(idx_to_char.values(), idx_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "data = np.array([char_to_idx[c] for c in text], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some of the functions we already know out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x.squeeze()\n",
    "    expx = np.exp(x - x.max())\n",
    "    return expx / expx.sum()\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    return sum([-np.log(p[t]) for p, t in zip(predictions, targets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(prev, curr, β):\n",
    "    return [\n",
    "        β * p + (1 - β) * c\n",
    "        for p, c\n",
    "        in zip(prev, curr)\n",
    "    ]\n",
    "    \n",
    "class AdamOptimizer:\n",
    "    def __init__(self, α=0.001, β1=0.9, β2=0.999, ϵ=1e-8):\n",
    "        self.α = α\n",
    "        self.β1 = β1\n",
    "        self.β2 = β2\n",
    "        self.ϵ = ϵ\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def send(self, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [0] * len(gradients)\n",
    "        if self.v is None:\n",
    "            self.v = [0] * len(gradients)\n",
    "\n",
    "        self.t += 1\n",
    "        αt = self.α * np.sqrt(1 - self.β2**self.t) / (1 - self.β1**self.t)\n",
    "        self.m = average(self.m, gradients, self.β1)        \n",
    "        self.v = average(self.v, (g*g for g in gradients), self.β2)\n",
    "\n",
    "        updates = [-αt * mi / (np.sqrt(vi) + self.ϵ) for mi, vi in zip(self.m, self.v)]\n",
    "        for upd in updates:\n",
    "            assert np.isfinite(upd).all()\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model\n",
    "\n",
    "- $x(t)$ is the $t$ character, one-hot encoded and a 1D array of length `vocab_size`.\n",
    "- $h(t)$ is the state of the hidden memory layer after seeing $t$ characters, encoded as a 1D array of numbers (neurons...)\n",
    "- $\\widehat y(t)$ is the prediction of the network after seeing $t$ characters, encoded as a 1D array of probabilities of length `vocab_size`\n",
    "\n",
    "The model is then written as:\n",
    "\n",
    "$$\n",
    "h(t) = \\tanh{\\big(x(t) \\cdot W_x^h + h(t-1) \\cdot W_h^h + b_h\\big)} \\\\\n",
    "\\widehat y(t) = softmax\\big(h(t) \\cdot W_h^y\\big)\n",
    "$$\n",
    "\n",
    "and we set $h(0) = (0, \\ldots, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will be performed by our `step` function.\n",
    "\n",
    "The `feed_forward` function will loop over a sequence of $x=(x_1, x_2, \\ldots, x_k)$ of some arbitray size - similar to batches in the FFN and CNN frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(params, x_t, h_t_1=None):\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    if h_t_1 is None:\n",
    "        h_t_1 = np.zeros(h_size)    \n",
    "    if h_t_1.ndim == 1:\n",
    "        h_t_1 = h_t_1.reshape(-1, 1)\n",
    "    if x_t.ndim == 1:\n",
    "        x_t = x_t.reshape(-1, 1)\n",
    "\n",
    "    # update hidden layer\n",
    "    h_t = np.tanh(Wxh @ x_t + Whh @ h_t_1 + bh)\n",
    "    # fully connected layer\n",
    "    z_t = Why @ h_t + by\n",
    "    z_t = z_t.squeeze()\n",
    "    h_t = h_t.squeeze()\n",
    "    # softmax readout layer\n",
    "    yhat_t = softmax(z_t)\n",
    "    return h_t, z_t, yhat_t\n",
    "\n",
    "def feed_forward(params, x, h0=None):\n",
    "    if h0 is None:\n",
    "        h0 = np.zeros(h_size)\n",
    "    h = {-1: h0}\n",
    "    \n",
    "    shape = (len(x), vocab_size)\n",
    "    x_original = x.copy()\n",
    "    x, z, yhat = np.zeros(shape), np.empty(shape), np.empty(shape)\n",
    "    \n",
    "    for t, char_idx in enumerate(x_original):\n",
    "        x[t, char_idx] = 1.0 # one-hot encoding input into xs  \n",
    "        h[t], z[t, :], yhat[t, :] = step(params, x[t, :], h[t-1])\n",
    "\n",
    "    return x, h, z, yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation and \"unrolling\" the network\n",
    "\n",
    "Back propagation works, as before, using the chain rule. \n",
    "It is similar to the [FFN example](FFN.ipynb), except that the $h$ layer adds a bit of complexity, but not much.\n",
    "\n",
    "The details of the gradient calculation can be found in Stanford's [\"Convolutional Neural Networks for Visual Recognition\" course](http://cs231n.github.io/neural-networks-case-study/#grad).\n",
    "\n",
    "What's important to discuss is that instead of back propagating a single step of the network $t$, we back propagate over a sequence of steps, that is over $x=(x_1, \\ldots, x_k)$ for some arbitrary $k$.\n",
    "\n",
    "![rolled RNN](img/rolled_rnn.png)\n",
    "\n",
    "How? By \"unrolling\" the network.\n",
    "\n",
    "![Unrolled RNN](img/unrolled_rnn.png)\n",
    "\n",
    "For example, for $k=3$, the input is $x=[x(1), x(2), x(3)]$, and we can write\n",
    "\n",
    "$$\n",
    "h(1) = \\tanh{\\big(x(1) \\cdot W_x^h + h(0) \\cdot W_h^h + b_h\\big)} \\\\\n",
    "\\widehat y(1) = softmax\\big(h(1) \\cdot W_h^y\\big) \\\\\n",
    "h(2) = \\tanh{\\big(x(2) \\cdot W_x^h + h(1) \\cdot W_h^h + b_h\\big)} \\\\\n",
    "\\widehat y(2) = softmax\\big(h(2) \\cdot W_h^y\\big) \\\\\n",
    "h(3) = \\tanh{\\big(x(3) \\cdot W_x^h + h(2) \\cdot W_h^h + b_h\\big)} \\\\\n",
    "\\widehat y(3) = softmax\\big(h(3) \\cdot W_h^y\\big)\n",
    "$$\n",
    "\n",
    "The cross entropy is computed by summing over all $\\widehat y(t)$ together, and then the gradient is computed for this cross entropy with respect to the various $W$ and $b$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(params, x, y, h0=None):\n",
    "    \"\"\"Calculates loss and gradiens of loss wrt paramters\n",
    "    \n",
    "    See http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of arrays\n",
    "        model parameters\n",
    "    x, y : list of integers\n",
    "        indices of characters for the input and target of the network\n",
    "    h0 : np.ndarray\n",
    "        initial hidden state of shape Hx1\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        value of loss function\n",
    "    dWxh, dWhh, dWhy, dbh, dby \n",
    "        gradients of the loss function wrt to model parameters\n",
    "    h0 : np.ndarray\n",
    "        initial hidden state\n",
    "    \"\"\"\n",
    "    n_inputs = len(x)\n",
    "    # forward pass: compute predictions and loss going forwards\n",
    "    x, h, z, yhat = feed_forward(params, x, h0=h0)\n",
    "    loss = cross_entropy(yhat, y)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    \n",
    "    # back propagate through the unrolled network\n",
    "    for t in reversed(range(len(y))):\n",
    "        # backprop into y\n",
    "        x_t, h_t, yhat_t, y_t = x[t], h[t], yhat[t], y[t] # can't zip because hs is not ordered\n",
    "        dyhat = yhat_t.copy()\n",
    "        dyhat[y_t] -= 1 # Yhat - Y\n",
    "        dWhy += np.outer(dyhat, h_t)  # outer product, same as dy.reshape(-1, 1) @ h.reshape(1, -1)\n",
    "        dby += dyhat.reshape(-1, 1) # dby is a column vector\n",
    "        # backprop into h_t\n",
    "        dh = Why.T @ dyhat + dh_next\n",
    "        # backprop through tanh\n",
    "        dh = (1 - h_t * h_t) * dh # tanh'(x) = 1-x^2\n",
    "        dbh += dh.reshape(-1, 1) # dbh is a column vector\n",
    "        dWxh += np.outer(dh, x_t)\n",
    "        dWhh += np.outer(dh, h[t-1]) # try to use h[t] instead of h[t-1] and see effect in grad_check\n",
    "        dh_next = Whh.T @ dh\n",
    "\n",
    "    gradients = dWxh, dWhh, dWhy, dbh, dby\n",
    "    for grad in gradients:\n",
    "        # clip to mitigate exploding gradients\n",
    "        np.clip(grad, -5, 5, out=grad) # out=grad makes this run in-place\n",
    "    return loss, gradients, h[n_inputs-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer is similar to previous trainers, it goes through the data in `seq_length` batches and then calculates gradients and updates parameters.\n",
    "`seq_length` is not just the batch size for the stochastic gradient descent but also the number of steps to \"unroll\" the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.optimizer = AdamOptimizer()\n",
    "        self.step, self.pos, self.h = 0, 0, None\n",
    "        self.seq_length = seq_length\n",
    "        self.data = data\n",
    "\n",
    "    def train(self, params):\n",
    "        self.step += 1\n",
    "        if self.pos + self.seq_length + 1 >= len(self.data):\n",
    "            # reset data position and hidden state\n",
    "            self.pos, self.h = 0, None\n",
    "        x = self.data[self.pos : self.pos + self.seq_length]\n",
    "        y = self.data[self.pos + 1 : self.pos + self.seq_length + 1]\n",
    "        \n",
    "        loss, gradients, self.h = back_propagation(params, x, y, self.h)\n",
    "        Δs = self.optimizer.send(gradients)\n",
    "        for par, Δ in zip(params, Δs):\n",
    "            par += Δ\n",
    "        self.pos += self.seq_length\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the network\n",
    "\n",
    "Finally, instead of a `predict` function, we have a `sample` function, which, given the parameters $W$s and $b$s), a seed char, number of chars, and an state for the hidden layer, produces a sample of text from the network.\n",
    "\n",
    "It does so by using the seed as $x(1)$ and drawing $x(t)$ for $t>1$ from the distribution given by $\\widehat y(t)$.\n",
    "\n",
    "![](img/sampling_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, seed_idx, num_samples, h0=None):\n",
    "    x = np.zeros((num_samples + 1, vocab_size), dtype=float)\n",
    "    x[0, seed_idx] = 1\n",
    "    idx = np.empty(num_samples, dtype=int)\n",
    "    h_t = h0\n",
    "    for t in range(num_samples):\n",
    "        h_t, _, yhat_t = step(params, x[t, :], h_t)\n",
    "        # draw from output distribution\n",
    "        idx[t] = np.random.choice(range(vocab_size), p=yhat_t.ravel())        \n",
    "        x[t + 1, idx[t]] = 1\n",
    "    chars = (idx_to_char[i] for i in idx)\n",
    "    return str.join('', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We can now initialize the parameters and meta-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 100 # number of units in hidden layer\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "max_train_step = 500000\n",
    "\n",
    "# initialize model parameters\n",
    "Wxh = np.random.randn(h_size, vocab_size) * 0.01 \n",
    "Whh = np.random.randn(h_size, h_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, h_size) * 0.01 \n",
    "bh = np.zeros((h_size, 1)) # hidden layer bias\n",
    "by = np.zeros((vocab_size, 1)) # readout layer bias\n",
    "params = Wxh, Whh, Why, bh, by\n",
    "\n",
    "trainer = Trainer(data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.\n",
      "It speak he villabs deen? Co come nown is onter like you he wis by the love his my know do's bey is the juagein that her my desbeent, at\n",
      "I can elengh ary tro'd hatd take day you.\n",
      "\n",
      "CALINFOLK:\n",
      "\n",
      "ARLAC\n",
      "\n",
      "train step 50000, loss: 51\n",
      "--------------------------------------------------------------------------------\n",
      "t lust that astine widol stilland:\n",
      "Their prineds, from day our pritur wouks sauth.\n",
      "\n",
      "MECHES:\n",
      "Were\n",
      "Of what before the severs swort; for his canto in patian: and sun the wor detwed;\n",
      "But sovertal hold her\n",
      "\n",
      "train step 100000, loss: 32\n",
      "--------------------------------------------------------------------------------\n",
      "ins:\n",
      "The hath sseplebours,\n",
      "Not throabsh his not beher withough I have neme'd\n",
      "I'll bart theeriss\n",
      "If therhome the waich\n",
      "To do be ard offolty desting\n",
      "this: my Lardiener:\n",
      "Yet though dispoes, What shom her\n",
      "\n",
      "train step 150000, loss: 42\n",
      "--------------------------------------------------------------------------------\n",
      "t bure of ow, like tursh forsiin;\n",
      "I am lod\n",
      "Dhersons,\n",
      "Then see the preseman,\n",
      "But are that have ablets of Somand\n",
      "And rues\n",
      "As know of the changer.\n",
      "\n",
      "Second;\n",
      "By all this withing; upting choeers woth me pra\n",
      "\n",
      "train step 200000, loss: 41\n",
      "--------------------------------------------------------------------------------\n",
      "t plinity blain?\n",
      "\n",
      "ILHOLUS:\n",
      "But ulas banr, Came and day's with ann went chopesfoll then in in gold.\n",
      "\n",
      "DRUTHHES:\n",
      "I am cerfuce ase a muswash as chamber wherefit. Wearring abore our ater.\n",
      "\n",
      "Firtheak it sinc\n",
      "\n",
      "train step 250000, loss: 49\n",
      "--------------------------------------------------------------------------------\n",
      "t san weak that falt strexertends. Poughter thess forriffal better I am:\n",
      "To rath fis, ib:\n",
      "Heis ot he bade and me, a do,\n",
      "Masee thyesed:\n",
      "Mosk to sweldnate dent\n",
      "be it is safe this\n",
      "To heart a would and he\n",
      "\n",
      "train step 300000, loss: 29\n",
      "--------------------------------------------------------------------------------\n",
      "t!f Stre\n",
      "ghought;\n",
      "The ampit was fear, and fightrabled and swear spirked untoll no seruster O's chises,\n",
      "Vouchafeness wiloge, and say\n",
      "The brition drable I lap whonencle\n",
      "Say it.\n",
      "\n",
      "CORIOLAT: I Fase you tax\n",
      "\n",
      "train step 350000, loss: 26\n",
      "--------------------------------------------------------------------------------\n",
      "tain!\n",
      "\n",
      "RonThas twim, gooder ungen all song the pivrreld he's my gently handly fouls fast nearge in a washern of have thee fand is diss is none?\n",
      "Thers hattrens her sing'd stale hape\n",
      "of light, I was cou\n",
      "\n",
      "train step 400000, loss: 29\n",
      "--------------------------------------------------------------------------------\n",
      "t grealn in distood morth'd with my fortuest,\n",
      "Bail.\n",
      "Now sthill pare well, whereffom the shanmanc our stid becks my whom a foils, un for uplance, what be it.\n",
      "\n",
      "COTONNAGOR:\n",
      "If I will not and or O: and my\n",
      "\n",
      "train step 450000, loss: 49\n",
      "--------------------------------------------------------------------------------\n",
      "t men dead, and cause other, and whingm all thee in thee\n",
      "I strainss;\n",
      "To be losd a' her penry'tr own,\n",
      "The gods lands, fell'd it gather:\n",
      "Then whose caloen oldiced with tourge is did green thee in your c\n",
      "\n",
      "train step 500000, loss: 33\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "while trainer.step < max_train_step:\n",
    "    loss = trainer.train(params)\n",
    "    if trainer.step % (max_train_step//10) == 0:\n",
    "        sample_text = sample(params, 0, 200)\n",
    "        print(sample_text)\n",
    "        print()\n",
    "        print('train step {:d}, loss: {:.2g}'.format(trainer.step, loss))\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- TensorFlow [RNN tutorial](https://www.tensorflow.org/tutorials/recurrent)\n",
    "- Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blogpost\n",
    "- [Obama-RNN](https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0) by samim.\n",
    "- [Gradient checking and advanced optimization](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization) on Stanford's \"Unsupervised Feature Learning and Deep Learning\" tutorial\n",
    "- [Making a Predictive Keyboard using Recurrent Neural Networks](http://curiousily.com/data-science/2017/05/23/tensorflow-for-hackers-part-5.html) by Venelin Valkov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com) and is part of the [_Data Science with Python_](https://github.com/yoavram/DataSciPy) workshop.\n",
    "\n",
    "The notebook was written using [Python](http://python.org/) 3.6.5.\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
