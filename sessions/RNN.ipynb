{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](../logo.png)\n",
    "\n",
    "# Recurrent Neural Networks\n",
    "## Yoav Ram\n",
    "\n",
    "In this session we will understand:\n",
    "- what recurrent neural network and how they work, and\n",
    "- how memory and state can be implemented in neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax 0.4.35 cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax \n",
    "import jax.numpy as np\n",
    "print('jax', jax.__version__, jax.default_backend())\n",
    "import optax # pip install optax\n",
    "\n",
    "from collections import Counter\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In developing this RNN we will follow [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/)'s [blogpost about RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness) ([original code gist](https://gist.github.com/karpathy/d4dee566867f8291f086) with BSD License).\n",
    "\n",
    "The data is just text data, in this case Shakespear's writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 99993\n",
      "Number of unique characters: 62\n",
      "Number of lines: 3298\n",
      "Number of words: 15893\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "That, poor contempt, or claim'd thou slept so faithful,\n",
      "I may contrive our father; and, in their defeated queen,\n",
      "Her flesh broke me and puttance of expedition house,\n",
      "And in that same that ever I lament this stomach,\n",
      "And he, nor Butly and my fury, knowing everything\n",
      "Grew daily ever, his great strength and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, that it may not pray their patience.'\n",
      "\n",
      "KING LEAR:\n",
      "The instant common maid, as we may less be\n",
      "a brave gentleman and joiner: he that finds u\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/shakespear.txt'\n",
    "with open(filename, 'rt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating \n",
    "- a list `chars` of the unique characters\n",
    "- `data_size` the number of total characters\n",
    "- `vocab_size` the number of unique characters\n",
    "- `int_to_char` a dictionary from index to char\n",
    "- `char_to_int` a dictionary from char to index\n",
    "and then we convert `data` from a string to a NumPy array of integers representing the chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "\n",
    "# char to int and vice versa\n",
    "int_to_char = dict(enumerate(chars)) #  == { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_int = dict(zip(int_to_char.values(), int_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "\n",
    "def onehot_encode(text):\n",
    "    ints = [char_to_int[c] for c in text]\n",
    "    ints = np.array(ints, dtype=int)\n",
    "    return jax.nn.one_hot(ints, vocab_size)\n",
    "\n",
    "def onehot_decode(data):\n",
    "    ints = data.argmax(axis=1).tolist()\n",
    "    chars = (int_to_char[k] for k in ints)\n",
    "    return str.join('', chars)\n",
    "\n",
    "X = onehot_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model\n",
    "\n",
    "- $x_t$ is the $t$ character, one-hot encoded and a 1D array of length `vocab_size`.\n",
    "- $h_t$ is the state of the hidden memory layer after seeing $t$ characters, encoded as a 1D array of numbers (neurons...)\n",
    "- $\\widehat y_t$ is the prediction of the network after seeing $t$ characters, encoded as a 1D array of probabilities of length `vocab_size`\n",
    "\n",
    "The model is then written as:\n",
    "\n",
    "The model is then written as:\n",
    "$$\n",
    "h_t = \\tanh{\\left(W_x^h x_t + W_h^h h_{t-1} + b_h\\right)}\n",
    "$$\n",
    "$$\n",
    "\\hat y_t = \\mathrm{softmax}\\left(W_h^y h_t + b_y\\right)\n",
    "$$\n",
    "$$\n",
    "x_{t+1} \\sim \\mathrm{Cat}(\\hat{y}_t)\n",
    "$$\n",
    "\n",
    "\n",
    "and we set $h_0 = (0, \\ldots, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will be performed by our `step` function.\n",
    "\n",
    "The `feed_forward` function will loop over a sequence of $x=(x_1, x_2, \\ldots, x_k)$ of some arbitray size - similar to batches in the FFN and CNN frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(params, x, h):\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    h = np.tanh(Wxh @ x + Whh @ h + bh)        \n",
    "    yhat = jax.nn.softmax(Why @ h + by)\n",
    "    return yhat, h\n",
    "\n",
    "def feed_forward(params, x, h):\n",
    "    yhat = np.zeros_like(x)\n",
    "    \n",
    "    for t in range(len(x)):\n",
    "        yhat_t, h = step(params, x[t], h)        \n",
    "        yhat = yhat.at[t, :].set(yhat_t) # equivalent to NumPy's yhat[t, :] = yhat_t\n",
    "\n",
    "    return yhat, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the network parameters so we can test `feed_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 100 # number of units in hidden layer\n",
    "\n",
    "def init_params(key):\n",
    "    subkeys = jax.random.split(key, 3)\n",
    "    Wxh = jax.random.normal(subkeys[0], (h_size, vocab_size)) * 0.01 \n",
    "    Whh = jax.random.normal(subkeys[1], (h_size, h_size)) * 0.01\n",
    "    Why = jax.random.normal(subkeys[2], (vocab_size, h_size)) * 0.01 \n",
    "    bh = np.zeros(h_size,) # hidden layer bias\n",
    "    by = np.zeros(vocab_size) # readout layer bias\n",
    "    params = Wxh, Whh, Why, bh, by\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.6 ms ± 2.43 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "odtvudbXQGo.X;vR,bvudQGo.\n",
      "hat, poor contempt, or cl\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(420) # generate new key based on the seed \"42\"\n",
    "params = init_params(key)\n",
    "\n",
    "x, y = X[:25], X[1:26]\n",
    "h = np.zeros(h_size)\n",
    "%timeit yhat, _ = feed_forward(params, x, h)\n",
    "yhat, _ = feed_forward(params, x, h)\n",
    "print(onehot_decode(yhat))\n",
    "print(onehot_decode(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation\n",
    "\n",
    "Back propagation works, as before, using the chain rule. \n",
    "It is similar to the [FFN example](FFN.ipynb), except that the $h$ layer adds a bit of complexity, but not much.\n",
    "\n",
    "The details of the gradient calculation can be found in Stanford's [\"Convolutional Neural Networks for Visual Recognition\" course](http://cs231n.github.io/neural-networks-case-study/#grad).\n",
    "\n",
    "What's important to discuss is that instead of back propagating a single step of the network $t$, we back propagate over a sequence of steps, that is over $x=(x_1, \\ldots, x_k)$ for some arbitrary $k$.\n",
    "\n",
    "How? By \"unrolling\" the network.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Hamid-Rabiee/publication/341956650/figure/fig1/AS:11431281078694336@1660200861643/An-RNN-unrolled-through-the-time-The-same-structure-is-repeated-at-adjacent-time-steps.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for sequence of length 3, the input is $x=[x_1, x_2, x_3]$, and we can write\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh{\\big(x_1 \\cdot W^{xh} + h_0 \\cdot W^{hh} + b_h\\big)} $$$$\n",
    "\\hat{y}_1 = \\text{softmax}\\big(h_1 \\cdot W^{hy}\\big) $$$$\n",
    "h_2 = \\tanh{\\big(x_2 \\cdot W^{xh} + h_1 \\cdot W^{hh} + b_h\\big)} $$$$\n",
    "\\hat{y}_2 = \\text{softmax}\\big(h_2 \\cdot W_h^y\\big) $$$$\n",
    "h_3 = \\tanh{\\big(x_3 \\cdot W^{xh} + h_2 \\cdot W^{hh} + b_h\\big)} $$$$\n",
    "\\hat{y}_3 = \\text{softmax}\\big(h_3 \\cdot W^{hy}\\big)\n",
    "$$\n",
    "\n",
    "The NLL or loss (negative log likelihood, cross entropy) for a single step is\n",
    "$$\n",
    "NLL = \\sum_{k=0}^{61}{x_{t+1,k} \\log{\\hat{y}_{t,k}}}\n",
    "$$\n",
    "where $k$ runs over all characters from 0 to 61 (for 62 characters).\n",
    "After unrolling the network, the NLL is computed by summing over all $\\hat{y}_t$ together, \n",
    "$$\n",
    "NLL = \\sum_{t=1}^{3}{\\sum_{k=0}^{61}{x_{t+1,k} \\log{\\hat{y}_{t,k}}}}\n",
    "$$\n",
    "The gradients are computed for this NLL with respect to the $W$ and $b$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.16748\n"
     ]
    }
   ],
   "source": [
    "def NLL(params, x, y, h):\n",
    "    yhat, h = feed_forward(params, x, h)    \n",
    "    loss = -(y * np.log(yhat)).sum()\n",
    "    return loss, h\n",
    "\n",
    "loss, h = NLL(params, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation with JAX\n",
    "Now instead of manually deriving the gradient and implementing it as a Python program, we use JAX's automatic differentiation. [`jax.grad`](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html#jax-first-transformation-grad) takes a function `f(a, b, c)` and returns a function `dfda(a, b, c)` that returns the gradient of `f` with respect to `a` at the values of `a`, `b`, and `c`. It does so by automating the procedure we did manually using the chain rule.\n",
    "\n",
    "In our case, `f` is `NLL`, `a` is `params`, and `b` and `c` are `x` and `y`, that is, we use `grad` on `NLL(params, x, y)` to get `backprop(params, x, y)`.\n",
    "\n",
    "The function [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html#value-and-grad) is used to return both `f(a,b,c)` (the \"value\") and the `dfda` (the \"grad\"). \n",
    "Finally, `has_aux` means that `f` return two values - the value that needs to be differentiated, and an auxillary value. In our case, the value to differentiate is `loss` and the auxillary is `h`. This is important because we need to keep track of `h` and `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop = jax.value_and_grad(NLL, has_aux=True)\n",
    "\n",
    "(loss, h), grads = backprop(params, x, y, h)\n",
    "for p, g in zip(params, grads):\n",
    "    assert p.shape == g.shape\n",
    "    assert not (g == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer with Optax\n",
    "\n",
    "We can use a JAX implementation of the Adam optimizer from the [Optax](https://optax.readthedocs.io/) library.\n",
    "We first create the optimizer and initialize its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.001) # 0.001 is the default from Kingma et al 2014\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the optimizer to compute the updates, and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, h), grads = backprop(params, x, y, h)\n",
    "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "params = optax.apply_updates(params, updates) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JITing the training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that does all this, and pass it to `jax.jit`, which [just-in-time compiles the function](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) so it can be executed efficiently in XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit # decreases runtime from 380 ms to <1 ms!\n",
    "def update_params(params, opt_state, x, y, h):\n",
    "    (loss, h), grads = backprop(params, x, y, h)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, h, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 µs ± 113 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit update_params(params, opt_state, x, y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.08142\n",
      "102.99289\n"
     ]
    }
   ],
   "source": [
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)\n",
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the network\n",
    "\n",
    "Finally, instead of a `predict` function, we have a `sample` function, which, given the parameters and the number of samples we want, produces a sample of text from the network.\n",
    "\n",
    "It does so by drawing a random seed for $x_0$ and drawing $x_t$ for $t>0$ from the distribution given by $\\widehat y_t$.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Aven-Zhou/publication/337006979/figure/fig3/AS:821430174380045@1572855623911/An-Illustration-of-the-Generating-Sequence-in-an-RNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'QRiQjqr\n",
      "HhvSvEJk!fq !-UEKIrGLbadVoO;:HWlZFfyh!cGtPRz\n",
      "WCJDdC :EBoiYHyM;Dvak FDG Li!zlaeBYkclFbVOm'PN\n"
     ]
    }
   ],
   "source": [
    "def sample(params, num_samples, key):\n",
    "    h = np.zeros(h_size)\n",
    "    \n",
    "    x = np.zeros((num_samples, vocab_size), dtype=float)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    seed_char = jax.random.choice(subkey, vocab_size)\n",
    "    x = x.at[0, seed_char].set(1)\n",
    "    \n",
    "    for t in range(1, num_samples):\n",
    "        yhat, h = step(params, x[t-1], h)\n",
    "        # draw from output distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        i = jax.random.choice(subkey, vocab_size, p=yhat)\n",
    "        x = x.at[t, i].set(1)\n",
    "    return onehot_decode(x)\n",
    "\n",
    "print(sample(params, 100, jax.random.key(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We setup the training - the sequence length to unroll the network, the number of batches, parameter initialization, Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "max_batches = 1000000\n",
    "h = np.zeros(h_size)\n",
    "pos = 0\n",
    "batch = 0 \n",
    "losses = []\n",
    "key = jax.random.key(8)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "params = init_params(subkey)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.001) # you can try with 0.01\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, loss 103.177963, pos 25\n",
      "\n",
      " l;HJCdR\n",
      "rr.BzkNsLdtu.xcABpQP'yOtBTB:nknaC Irzl\n",
      "'IWX yJM-ZTVRQHtjBLkeyu;kbcObr-.P?ztjKpED gxjHiA\n",
      ";tXUVFb:fbzcu\n",
      "\n",
      "Y,ysuPDYTNdq ?m!\n",
      "WGbFHzQLl,MacrAqngQkFV-TDsd\n",
      "CREJjtHcQYytIiOaZRHUYvOPR,QnufWwAmVxsE ibdi\n",
      "--------------------------------------------------------------------------------\n",
      "batch 100000, loss 31.441458, pos 650\n",
      "\n",
      "JoX:\n",
      "Straight has, what, my in stact purfeand.\n",
      "\n",
      "HOLON:\n",
      "I why cwerigh's thilks Endees I did upfice undoyos intain,\n",
      "With majuce and not she menter in'tinnse come your voice\n",
      "To see.\n",
      "sine streaths expire.\n",
      "--------------------------------------------------------------------------------\n",
      "batch 200000, loss 38.215614, pos 1275\n",
      "\n",
      "Thinst bureand you speak, but hother Midgnt death; thy spirit---chart fespity: and will day whilus lord her proforitle not, that so muct had ingespet, must brought:\n",
      "Yad this me dote you are renoullawe\n",
      "--------------------------------------------------------------------------------\n",
      "batch 300000, loss 36.385902, pos 1900\n",
      "\n",
      "O SI? Eup: but I say in this in all.\n",
      "\n",
      "ISNYOP:\n",
      "But in rezart command bad; and hath ses oclland, if nother,\n",
      "And eyandst from at ousent,\n",
      "This not suntreds to ses used-s? a frave ibsperd in a staker a bri\n",
      "--------------------------------------------------------------------------------\n",
      "batch 400000, loss 36.833721, pos 2525\n",
      "\n",
      ":\n",
      "Yet's hearts but danple to not joy to me\n",
      "Staith been tane\n",
      "forcusy creater;\n",
      "And wen: on Alody inch with whicks day\n",
      "To was cirnetue himselfit: something in the kingthat son on the sea you\n",
      "mad. That se\n",
      "--------------------------------------------------------------------------------\n",
      "batch 500000, loss 20.836918, pos 3150\n",
      "\n",
      "K ANt IF AVwzLL Duke outhrate conow, livesteeds of the galliance I do naw you are the childrude made his call fursow in the ceiends, is, 'Tllage not sent he a speevily every kream news my from and end\n",
      "--------------------------------------------------------------------------------\n",
      "batch 600000, loss 42.478249, pos 3775\n",
      "\n",
      "x'G on time ey heaven used outh, need;\n",
      "It she wile, have more, the sist, to stive of Must\n",
      "I them and I have\n",
      "To the speet to\n",
      "trally isiged be more not\n",
      "Than singlediness an Stop thousardon stand, my lor\n",
      "--------------------------------------------------------------------------------\n",
      "batch 700000, loss 54.678734, pos 4400\n",
      "\n",
      "zend mighter, you he, liken queed! O, my fury inkingude my bontliet to with the breast as 'le stroke yet between wadle of mad, to gies and them would dot?\n",
      "\n",
      "LAU GIPH:\n",
      "No:\n",
      "Now, A enndere.\n",
      "\n",
      "DUKE OF:\n",
      "To n\n",
      "--------------------------------------------------------------------------------\n",
      "batch 800000, loss 36.809277, pos 5025\n",
      "\n",
      "th, I thought any hustel your way,\n",
      "Loeds I speerly held.\n",
      "Orill here, though than to dead, Gother's liegft of a fraidens then know rentlmealy, Fisber, hour thruhb'd bent insent had fear shall praused s\n",
      "--------------------------------------------------------------------------------\n",
      "batch 900000, loss 35.423103, pos 5650\n",
      "\n",
      " doth asch a'm rever sis lought bring\n",
      "Lut rast foily toot\n",
      "As Crack.\n",
      "\n",
      "JALIO:\n",
      "Be deary be false is thy ever;\n",
      "Fors, and kidh a kinn. Thy will seep in I thouse ender and stonewick-madet heard weep remicst\n",
      "--------------------------------------------------------------------------------\n",
      "batch 1000000, loss 32.705639, pos 6275\n",
      "\n",
      "ker uses.\n",
      "Half dotite, that I be not store.\n",
      "\n",
      "BANQUODIUS OF YORWOLL:\n",
      "For Gousors,\n",
      "Mood then\n",
      "Thee, I\n",
      "joy\n",
      "Master bent thy Juch as in expodon I'll be out my\n",
      "seslood forth, I'll make me!\n",
      "Come, contement:el\n",
      "--------------------------------------------------------------------------------\n",
      "CPU times: user 32min 35s, sys: 16min 30s, total: 49min 5s\n",
      "Wall time: 1h 8min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while batch <= max_batches:\n",
    "    if pos + seq_length + 1 >= data_size:\n",
    "        # reset data position and hidden state\n",
    "        pos, h = 0, np.zeros(h_size) \n",
    "        \n",
    "    x = X[pos : pos + seq_length]\n",
    "    y = X[pos + 1 : pos + seq_length + 1]\n",
    "    pos += seq_length\n",
    "        \n",
    "    params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if batch % (max_batches // 10) == 0:\n",
    "        print('batch {:d}, loss {:.6f}, pos {}'.format(batch, loss, pos))\n",
    "        print()\n",
    "        \n",
    "        key, subkey = jax.random.split(key)        \n",
    "        sample_text = sample(params, 200, subkey)\n",
    "        print(sample_text)\n",
    "        print('-'*80)\n",
    "    batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blogpost\n",
    "- Vinyals et al. (2014) [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555).\n",
    "- [Obama-RNN](https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0) by samim.\n",
    "- [Making a Predictive Keyboard using Recurrent Neural Networks](http://curiousily.com/data-science/2017/05/23/tensorflow-for-hackers-part-5.html) by Venelin Valkov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com).\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
