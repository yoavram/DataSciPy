{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](img/logo.png)\n",
    "\n",
    "# Logistic Model\n",
    "## Yoav Ram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "blue, green, red = plt.color_sequences['Set1'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we used one feature with integer values to predict another integer value.\n",
    "What if we want to predict a category or class (i.e. classify or categorize) instead of predicting a number?\n",
    "\n",
    "There are many ways to classify data (even without a training set), and one of the most common is **logistic regression**. \n",
    "But *regression* is usually used for predicting real numbers, how is regression related to classification?\n",
    "In logistic regression we are trying to regress (predict a real number) the probability of some data being in a one class and not the other. \n",
    "Logistic regression is binomial (two classes, one free variable) but it can easily be expanded to *multinomial logistic regression*, sometimes also known as *softmax regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get me the data\n",
    "\n",
    "Let's start with a concrete example.\n",
    "We'll get some data related to the Titanic (courtesy of the [IPython Cookbook](http://ipython-books.github.io/) by Cyrille Rossant, originally from the [kaggle Titanic challenge](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/320px-RMS_Titanic_3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../data/titanic.zip'\n",
    "titanic_zip = zipfile.ZipFile(filename)\n",
    "with titanic_zip.open('data/titanic_train.csv') as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's just work with three features (not just one) - `Sex`, `Age` and `Pclass`.\n",
    "We will treat `Sex` as a category (0 or 1), `Age` as a number (positive intger), `Pclass` as another number, because it is the ticket class.\n",
    "\n",
    "We will try to predict the `Survived` variable, which is simply 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex   Age  Pclass  Survived\n",
       "0    0  22.0       3         0\n",
       "1    1  38.0       1         1\n",
       "2    1  26.0       3         1\n",
       "3    1  35.0       1         1\n",
       "4    0  35.0       3         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Sex', 'Age', 'Pclass', 'Survived']].copy() # what if I don't put .copy?\n",
    "df['Sex'] = df['Sex'] == 'female' # convert to boolean\n",
    "df['Sex'] = df['Sex'].astype(int) # then convert to int\n",
    "df = df.dropna() # remove rows with \"not a number\" elements\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Let's try and use logistic regression to clear this up (if we can).\n",
    "How does it work?\n",
    "\n",
    "We briefly mentioned that when predicting integer values, the normal distribution, and hence the *normal linear model*, is not be the best model, and demonstrated that a [GLM](https://en.wikipedia.org/wiki/Generalized_linear_model) with a Poisson distribution and a log link function intead of a normal distribution performed better.\n",
    "\n",
    "We will do a similar trick here.\n",
    "\n",
    "We first use a linear model (as we did before) to predict the **log-odds** for survival.\n",
    "\n",
    "Odds here is actually short for odds-ratio (OR), which is just the ratio of the probability that something happens and the probability that it does not happen:\n",
    "$$\n",
    "OR = \n",
    "\\frac{P(\\text{Survived})}{P(\\text{Died})}\n",
    "$$\n",
    "so when the odds-ratio is 1, both events are as likley, and when it is >1 (<1) survival (death) is more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the odds-ratio instead of the probability itself, because it is a value between 0 and $\\infty$, rather then between 0 and 1, which is important both for mathematical formality, as the linear model is unbounded, and for interpretation - the odds can be doubles again and again (2:1 becoming 4:1 becomnig 8:1...) whereas the probability cannot (what is the double of 75%?).\n",
    "\n",
    "The log-odds, which we mark as $z$, is the natural logarithm of the odds ratio.\n",
    "$$\n",
    "z = \n",
    "\\log{\\frac{P(\\text{Survived})}{P(\\text{Died})}} = \n",
    "\\log{\\frac{P(y=1)}{P(y=0)}}\n",
    "$$\n",
    "Why use the log-odds? Because (i) it is more mathematically convinient, as log-odds is symmetric in $P(..)$, whereas odds is not, and (ii) it is easier to interpret, as we will see below.\n",
    "\n",
    "So when the log-odds-ratio is 0, both events are as likley, and when it is positive (negative) survival (death) is more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to use a linear model for predicting the log-odds, we have $m$ features, $x_1, x_2, \\ldots, x_m$, and we try to estimate coefficients $a_0, a_1, \\ldots, a_m$ such that\n",
    "$$\n",
    "z = a_0 + a_1 x_1 + \\ldots + a_m x_m\n",
    "$$\n",
    "gives us a good estimation of the real log-odds.\n",
    "\n",
    "But we want to estimate $\\widehat{y}=P(y=1)$. Another nice feature of the log-odds is that it is invertible: from the log-odds we can find the probability for the event to occur using the *logisitic* (hence the name of the method!) which is also called the *expit* function:\n",
    "$$\n",
    "\\widehat{y} = P(y=1) = expit(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Sex', 'Age', 'Pclass']\n",
    "X = df[features].values\n",
    "Y = df['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds(X, a):\n",
    "    Z = X @ a\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53. 80. 62. 74. 79.]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "print(logodds(X, a)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood\n",
    "\n",
    "Now we want to find suitable $a_i$ such that we make a good prediction. \n",
    "We'll use *maximum likelihood* again.\n",
    "\n",
    "Given data $(x, y)$ where $x=(x_1, x_2, x_3)$ are some numbers and $y$ is either 0 or 1, the logistic model provides us an estimate $\\widehat y$ for the probability that $y=1$:\n",
    "\n",
    "$$\n",
    "\\widehat{y} = P(\\text{Survived})= P(y=1)=\\frac{1}{1+e^{-z}} = \n",
    "\\frac{1}{1+e^{-a_1 x_1 - a_2 x_2 -a_3 x_3}}\n",
    "$$\n",
    "\n",
    "The distribution of $y$ here is very straightforward - the Bernoulli distribution with success probability $\\widehat{y}$, which equals to Binomial distribution with number of trials $n=1$ and success probability $\\widehat{y}$:\n",
    "\n",
    "$$\n",
    "y \\sim Bin(1, \\widehat{y})\n",
    "$$\n",
    "\n",
    "The likelihood of this model is\n",
    "\n",
    "$$\n",
    "\\mathbf{L}(a_1, a_2, a_3 \\mid x_1, x_2, x_3, y) = \n",
    "P(y \\mid a_1, a_2, a_3, x_1, x_2, x_3) = \n",
    "\\cases{\n",
    "    \\widehat{y}, & y=1 \\\\\n",
    "    1-\\widehat{y}, & y=0\n",
    "}\n",
    "$$\n",
    "\n",
    "If we have many $(x,y)$ pairs, and we will **assume that each pair is independent** (which maybe we can't always do, and specifically in the Titanic case we probably shouldn't do, but ok) then the joint likelihood of all the pairs is just the product of all the pair likelihoods: the product is used because the joint probability of independent events occuring is the product of their occurence probabilities.\n",
    "Writing the set of $x$s as $X$ and the corresponding set of $y$s as $Y$, and because $y$ are either 0 or 1,\n",
    "\n",
    "$$\n",
    "\\mathbf{L}(a_1, a_2, a_3 \\mid X, Y) = \n",
    "\\prod_{i=1}^{n} {(\\widehat{y}_i)^{y_i} \\; (1-\\widehat{y}_i) ^{1-y_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the log-likelihood because otherwise we will have to deal with a product of really small numbers; so we take the sum of the log of the likelihood $\\mathbf{L}$ of the the $(x, y)$ pairs (sum because the log of products is the sum of logs). The use of log here is not \"magic\", it's a mathematical convenience. It just happens that \"log-likelihood\" sounds very impressive.\n",
    "\n",
    "$$\n",
    "\\log{\\mathbf{L}(a_1, a_2, a_3 \\mid X, Y)} = \n",
    "\\sum_{i=1}^{n} {y_i \\log{\\widehat{y}_i} + (1-y_i) \\log{(1-\\widehat{y}_i)}}\n",
    "$$\n",
    "\n",
    "This is very similar to the negative of an information theory function called [*cross entropy*](https://en.wikipedia.org/wiki/Cross_entropy), and we usually average it over all the samples so that we can compare cross entropies between datasets of different size:\n",
    "\n",
    "$$\n",
    "\\mathbf{J}(a_1, a_2, a_3, X, Y) = -\\frac{1}{n} \\log{\\mathbf{L}(a_1, a_2, a_3 \\mid X, Y)}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of samples in $X,Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical stability\n",
    "\n",
    "Due to numerical issues when using very small or very large numbers, we should play around with the definition a little bit to find an expression that we can calculate with good numerical stability.\n",
    "\n",
    "Note that\n",
    "$$\n",
    "\\log{\\widehat{y}} = \n",
    "\\log{\\Big(\\frac{1}{1 + e^{-z}}\\Big)}  = \n",
    "-\\log{\\Big(1 + e^{-z}\\Big)} \\\\\n",
    "\\log{(1-\\widehat{y})} = \n",
    "\\log{\\Big(\\frac{e^{-z}}{1 + e^{-z}}\\Big)} = \n",
    "\\log{(e^{-z})} - \\log{({1 + e^{-z}})} =\n",
    "= -z - \\log{\\Big(1 + e^{-z}\\Big)}\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "y \\log{\\widehat{y}} + (1-y) \\log{(1-\\widehat{y})} = \\\\\n",
    "-y \\log{\\Big(1 + e^{-z}\\Big)} + (1-y)\\Big(-z - \\log{\\Big(1 + e^{-z}\\Big)}\\Big) = \\\\\n",
    "(1-y)z - \\log{\\Big(1 + e^{-z}\\Big)}\n",
    "$$\n",
    "\n",
    "Finally,\n",
    "$$\n",
    "\\log{\\mathbf{L}(a_1, a_2, a_3 \\mid X, Y)} = \n",
    "\\sum_{i=1}^{n} {-z_i (1-y_i) - \\log{\\Big(1 + e^{-z_i}\\Big)}},\n",
    "$$\n",
    "where $z_i = a_1 x_{i,1} + a_2 x_{i,2} + a_n x_{i,3}$ and $i$ is the sample index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: cross-entropy\n",
    "\n",
    "Implement the cross entropy function.\n",
    "\n",
    "**Note: the subsequent `%run ...` line will load the instructor solution; to use your solution, comment this line out with `#`**.\n",
    "\n",
    "**Reminder**\n",
    "- Edit cell by double clicking\n",
    "- Run cell by pressing _Shift+Enter_\n",
    "- Get autocompletion by pressing _Tab_\n",
    "- Get documentation by pressing _Shift+Tab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(a, X, Y):\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.89217186890601\n"
     ]
    }
   ],
   "source": [
    "print(cross_entropy(a, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent and the chain rule\n",
    "\n",
    "Now we can minimize the cross entropy using gradient descent.\n",
    "We need to calculate the derivative of the cross entropy with regards to $a_i$.\n",
    "We will use the [chain rule](https://en.wikipedia.org/wiki/Chain_rule): \n",
    "\n",
    "$$\n",
    "f(g(x))' = f'(g(x)) \\cdot g'(x), \n",
    "$$\n",
    "\n",
    "which is easier to write as \n",
    "\n",
    "$$\n",
    "\\frac{dx}{dy} = \\frac{dx}{dz} \\cdot \\frac{dz}{dy}\n",
    "$$\n",
    "\n",
    "because then we can eliminate fractions as if these were fractions and not [infinitesimals](https://en.wikipedia.org/wiki/Infinitesimal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that $z=\\text{log-odds} = a_1 x_1 + a_2 x_2 + a_3 x_3$ and $\\mathbf{J}$ is the cross entropy function which we want to minimize.\n",
    "\n",
    "Then for a single sample $(x, y)$:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial a_k} = \n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\widehat y} \\cdot \\frac{\\partial \\widehat y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial a_k}\n",
    "$$\n",
    "\n",
    "The easiest one is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial a_k} = x_k\n",
    "$$\n",
    "\n",
    "The derivative of the logistic function is (you can verify later):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\widehat y}{\\partial z} = \\widehat y ( 1-\\widehat y )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, because $\\frac{d}{dx} log(x) = \\frac{1}{x}$ (again, you can verify this),\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\widehat y} = \n",
    "- \\frac{\\partial}{\\partial \\widehat y} \\big(y \\log{\\widehat{y}} + (1-y) \\log{(1-\\widehat{y})}\\big) = $$$$\n",
    "-y \\cdot \\frac{1}{\\widehat y} + (1-y) \\cdot \\frac{1}{1-\\widehat y} = $$$$\n",
    "\\frac{\\widehat y - y}{\\widehat y ( 1 - \\widehat y)}\n",
    "$$\n",
    "\n",
    "Putting it all together,\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial a_k} = \n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\widehat y} \\cdot \\frac{\\partial \\widehat y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial a_k} = $$$$\n",
    "\\frac{\\widehat y - y}{\\widehat y ( 1 - \\widehat y)} \\cdot \\widehat y ( 1-\\widehat y ) \\cdot x_k = $$$$\n",
    "(\\widehat y - y) \\cdot x_k\n",
    "$$\n",
    "\n",
    "which you have to admit is pretty cool: this is the residual (i.e. difference between the predicted and oberverd probabilities, $\\widehat y - y$), so 0 when you got it right and 1 or -1 when you got it completely wrong, multiplied by the stength of the signal, so that strong signals (large $x_k$) have a stonger gradient and stonger effect on the result.\n",
    "\n",
    "This was the gradient for a single sample. We average it over all samples to get a good estimate of the \"real gradient\" (law of large numbers etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: gradient descent\n",
    "\n",
    "Implement a `gradient_descent(X, Y, a, η)` function, similar to the one we had in the [linear model session](linear-model.ipynb), which returns updated values for the coefficients based on one iteration of the gradient descent algorithm.\n",
    "Use the above derivation to calculate the gradients and return updated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(a, X, Y, η=0.01):\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your solution is ready, comment out the first line of the next cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99910365 1.81813039 2.98523828]\n"
     ]
    }
   ],
   "source": [
    "print(gradient_descent(a, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model is done in much the same way as we did with the linear model - just have to choose initial coefficeints, different stopping condition, and adjust to the API of the new `gradient_descent` function.\n",
    "\n",
    "This time we stop when the difference in cross entropy between two iterations is smaller than some value ($10^{-4}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model(X, Y, a=None, iters=50000, PRINT=True):\n",
    "    if a is None:\n",
    "        a = np.ones(X.shape[1])\n",
    "    for t in range(iters+1):\n",
    "        a = gradient_descent(a, X, Y)\n",
    "        if t % (iters//10) == 0 and PRINT:\n",
    "            print(\"{:5d}: loss={:.6f}, a={}\".format(t, cross_entropy(X, Y, a), a))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0: loss=501.003293, a=[0.99910465 0.81814023 0.98525564]\n",
      " 5000: loss=2946.797819, a=[ 2.45084731 -0.00336839 -0.61851448]\n",
      "10000: loss=2941.703165, a=[ 2.61310454 -0.00345444 -0.6479314 ]\n",
      "15000: loss=2940.914194, a=[ 2.63827447 -0.00346145 -0.6525979 ]\n",
      "20000: loss=2940.788131, a=[ 2.64229717 -0.00346241 -0.65334631]\n",
      "25000: loss=2940.767889, a=[ 2.64294312 -0.00346256 -0.65346656]\n",
      "30000: loss=2940.764636, a=[ 2.64304692 -0.00346259 -0.65348588]\n",
      "35000: loss=2940.764113, a=[ 2.6430636  -0.00346259 -0.65348899]\n",
      "40000: loss=2940.764029, a=[ 2.64306628 -0.00346259 -0.65348949]\n",
      "45000: loss=2940.764016, a=[ 2.64306671 -0.00346259 -0.65348957]\n",
      "50000: loss=2940.764014, a=[ 2.64306678 -0.00346259 -0.65348958]\n"
     ]
    }
   ],
   "source": [
    "a = logistic_model(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odds-ratios:\n",
      "[14.05624498  0.9965434   0.52022723]\n"
     ]
    }
   ],
   "source": [
    "print(\"Odds-ratios:\")\n",
    "print(np.exp(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret these results as follows:\n",
    "- women were much more likely to survive\n",
    "- older passengers were less likely to survive\n",
    "- passengers with expensive tickets were much move likely to survive\n",
    "\n",
    "but remember the correlation we found between Age and Pclass...\n",
    "\n",
    "What about the accuracy of our model predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(a, X, Y):\n",
    "    nsamples = Y.shape[0]\n",
    "    \n",
    "    Z = X @ a\n",
    "    Yhat = 1 / (1 + np.exp(-Z))\n",
    "    return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7801120448179272\n"
     ]
    }
   ],
   "source": [
    "Yhat = predict(a, X, Y)\n",
    "predictions = Yhat > 0.5\n",
    "print(\"Accuracy: {}\".format((predictions == Y).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-the-shelf solutions\n",
    "\n",
    "Of course, we should use solutions that someone else already optimized and debugged.\n",
    "\n",
    "## Scikit-learn\n",
    "\n",
    "[scikit-learn](http://scikit-learn.org/stable/index.html) is an awesone machine learning package will very good documentation.\n",
    "Note that the package name is `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7801120448179272"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept=False)\n",
    "model.fit(X, Y)\n",
    "model.score(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels\n",
    "\n",
    "[Statsmodels](http://statsmodels.sourceforge.net/) is also a good package, more oriented to statistics then machine learning.\n",
    "\n",
    "Note that statsmodels uses IRLS to find the maximum likelihood, rather than gradient descent, and therefore it is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.476553\n",
      "         Iterations 6\n",
      "Accuracy: 0.7801120448179272\n"
     ]
    }
   ],
   "source": [
    "logit = sm.Logit(Y, X)\n",
    "result = logit.fit()\n",
    "Yhat = result.predict(X)\n",
    "predictions = Yhat > 0.5\n",
    "print(\"Accuracy: {}\".format((predictions == Y).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>   714</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   711</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 27 Jun 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.2944</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>09:36:43</td>     <th>  Log-Likelihood:    </th> <td> -340.26</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -482.26</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.141e-62</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    2.6431</td> <td>    0.193</td> <td>   13.710</td> <td> 0.000</td> <td>    2.265</td> <td>    3.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>   -0.0035</td> <td>    0.004</td> <td>   -0.777</td> <td> 0.437</td> <td>   -0.012</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>   -0.6535</td> <td>    0.072</td> <td>   -9.108</td> <td> 0.000</td> <td>   -0.794</td> <td>   -0.513</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &        y         & \\textbf{  No. Observations:  } &      714    \\\\\n",
       "\\textbf{Model:}           &      Logit       & \\textbf{  Df Residuals:      } &      711    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        2    \\\\\n",
       "\\textbf{Date:}            & Thu, 27 Jun 2024 & \\textbf{  Pseudo R-squ.:     } &   0.2944    \\\\\n",
       "\\textbf{Time:}            &     09:36:43     & \\textbf{  Log-Likelihood:    } &   -340.26   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -482.26   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } & 2.141e-62   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "            & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{x1} &       2.6431  &        0.193     &    13.710  &         0.000        &        2.265    &        3.021     \\\\\n",
       "\\textbf{x2} &      -0.0035  &        0.004     &    -0.777  &         0.437        &       -0.012    &        0.005     \\\\\n",
       "\\textbf{x3} &      -0.6535  &        0.072     &    -9.108  &         0.000        &       -0.794    &       -0.513     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  714\n",
       "Model:                          Logit   Df Residuals:                      711\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Thu, 27 Jun 2024   Pseudo R-squ.:                  0.2944\n",
       "Time:                        09:36:43   Log-Likelihood:                -340.26\n",
       "converged:                       True   LL-Null:                       -482.26\n",
       "Covariance Type:            nonrobust   LLR p-value:                 2.141e-62\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             2.6431      0.193     13.710      0.000       2.265       3.021\n",
       "x2            -0.0035      0.004     -0.777      0.437      -0.012       0.005\n",
       "x3            -0.6535      0.072     -9.108      0.000      -0.794      -0.513\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Pawitan Y, 2001. *In all likelihood: statistical modelling and inference using likelihood*. **Ch. 6.2**.\n",
    "- Cyrille Rossant, 2014. [IPython Interactive Computing and Visualization Cookbook](https://ipython-books.github.io/cookbook/) (a similar Titanic example is shown in chapter \"Predicting who will survive on the Titanic with logistic regression\"). \n",
    "- Scikit-learn documentation has a [tutorial](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html#classification) using the classical Iris dataset, with examples for other classification methods other than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com).\n",
    "\n",
    "This work is licensed under a CC BY-NC-SA 4.0 International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
