{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](img/logo.png)\n",
    "\n",
    "# Recurrent Neural Networks\n",
    "## Yoav Ram\n",
    "\n",
    "In this session we will understand:\n",
    "- what recurrent neural network and how they work, and\n",
    "- how memory and state can be implemented in neural networks\n",
    "- how JAX can be used for high-performance numerical computing as a NumPy replacement\n",
    "\n",
    "**Please use the correct kernel**: in the notebook menu bar, click `Kernel`, then `Change kernel...` then choose `conda_tensorflow2_p37`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax # http://jax.readthedocs.io\n",
    "import jax.numpy as np\n",
    "import optax # https://optax.readthedocs.io\n",
    "\n",
    "from collections import Counter\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX\n",
    "\n",
    "[JAX](https://jax.readthedocs.io/en/latest/index.html) combines automatic differentiation and a machine-learning specific compiler ([XLA](https://www.tensorflow.org/xla)) for high-performance numerical computing.\n",
    "- JAX provides a familiar NumPy-style API for ease of adoption,\n",
    "- JAX includes composable function transformations for compilation, batching (i.e. vectorization), automatic differentiation, and parallelization,\n",
    "- The same code executes on multiple backends, including CPU, GPU, and TPU (Google's GPU)\n",
    "\n",
    "When using JAX we can mostly use the NumPy API, with some important difference:\n",
    "- JAX arrays are immutable so we cannot use item assignment\n",
    "- random number generations requires us to provide a random key at every call (the random number generator is stateless)\n",
    "\n",
    "JAX allows us to just-in-time compile functions and importantly to compute gradients automatically. We will see these features as we proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In developing this RNN we will follow [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/)'s [blogpost about RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness) ([original code gist](https://gist.github.com/karpathy/d4dee566867f8291f086) with BSD License).\n",
    "\n",
    "The data is just text data, in this case Shakespear's writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 99993\n",
      "Number of unique characters: 62\n",
      "Number of lines: 3298\n",
      "Number of words: 15893\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "That, poor contempt, or claim'd thou slept so faithful,\n",
      "I may contrive our father; and, in their defeated queen,\n",
      "Her flesh broke me and puttance of expedition house,\n",
      "And in that same that ever I lament this stomach,\n",
      "And he, nor Butly and my fury, knowing everything\n",
      "Grew daily ever, his great strength and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, that it may not pray their patience.'\n",
      "\n",
      "KING LEAR:\n",
      "The instant common maid, as we may less be\n",
      "a brave gentleman and joiner: he that finds u\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/shakespear.txt'\n",
    "with open(filename, 'rt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating \n",
    "- a list `chars` of the unique characters\n",
    "- `data_size` the number of total characters\n",
    "- `vocab_size` the number of unique characters\n",
    "- `int_to_char` a dictionary from index to char\n",
    "- `char_to_int` a dictionary from char to index\n",
    "and then we convert `data` from a string to a NumPy array of integers representing the chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "\n",
    "# char to int and vice versa\n",
    "int_to_char = dict(enumerate(chars)) #  == { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_int = dict(zip(int_to_char.values(), int_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "\n",
    "def onehot_encode(text):\n",
    "    ints = [char_to_int[c] for c in text]\n",
    "    ints = np.array(ints, dtype=int)\n",
    "    return jax.nn.one_hot(ints, vocab_size)\n",
    "\n",
    "def onehot_decode(data):\n",
    "    ints = data.argmax(axis=1).tolist()\n",
    "    chars = (int_to_char[k] for k in ints)\n",
    "    return str.join('', chars)\n",
    "\n",
    "X = onehot_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model\n",
    "\n",
    "- $x_t$ is the $t$ character, one-hot encoded and a 1D array of length `vocab_size`.\n",
    "- $h_t$ is the state of the hidden memory layer after seeing $t$ characters, encoded as a 1D array of numbers (neurons...)\n",
    "- $\\widehat y_t$ is the prediction of the network after seeing $t$ characters, encoded as a 1D array of probabilities of length `vocab_size`\n",
    "\n",
    "The model is then written as:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh{\\big(x_t \\cdot W_x^h + h_{t-1} \\cdot W_h^h + b_h\\big)}\n",
    "$$\n",
    "$$\n",
    "\\widehat y_t = softmax\\big(h_t \\cdot W_h^y + b_y\\big)\n",
    "$$\n",
    "\n",
    "and we set $h_0 = (0, \\ldots, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will be performed by our `step` function.\n",
    "\n",
    "The `feed_forward` function will loop over a sequence of $x=(x_1, x_2, \\ldots, x_k)$ of some arbitray size - similar to batches in the FFN and CNN frameworks.\n",
    "The loss function `cross_entropy` is computed from the parameters and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(params, x, h):\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    h = np.tanh(Wxh @ x + Whh @ h + bh)        \n",
    "    yhat = jax.nn.softmax(Why @ h + by) # softmax function implemented in JAX\n",
    "    return yhat, h\n",
    "    \n",
    "def feed_forward(params, x, h):\n",
    "    yhat = np.zeros_like(x)\n",
    "    \n",
    "    for t in range(len(x)):\n",
    "        yhat_t, h = step(params, x[t], h)\n",
    "        # this is the JAX syntax that replaces NumPy item assignment\n",
    "        yhat = yhat.at[t, :].set(yhat_t) # equivalent to yhat[t, :] = yhat_t\n",
    "\n",
    "    return yhat, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the network parameters so we can test `feed_forward`.\n",
    "\n",
    "Because [JAX pseudo-random number generation](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) works differently than NumPy -- the generator is stateless, so we need to provide a \"random key\" (i.e. seed) at every function call. Random functions consume the key, but do not modify it. Feeding the same key to a random function will always result in the same sample being generated. To generate different and independent samples, we must `split()` the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 100 # number of units in hidden layer\n",
    "\n",
    "def init_params(key):\n",
    "    key, subkey = jax.random.split(key) # split the key - one for consumption and one for the rest of the program\n",
    "    Wxh = jax.random.normal(subkey, (h_size, vocab_size)) * 0.01 \n",
    "    key, subkey = jax.random.split(key) # split the key...\n",
    "    Whh = jax.random.normal(subkey, (h_size, h_size)) * 0.01\n",
    "    key, subkey = jax.random.split(key) # split the key\n",
    "    Why = jax.random.normal(subkey, (vocab_size, h_size)) * 0.01 \n",
    "    bh = np.zeros(h_size,) # hidden layer bias\n",
    "    by = np.zeros(vocab_size) # readout layer bias\n",
    "    params = Wxh, Whh, Why, bh, by\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y'JPTT:vJ'TUvQs:R:sTTv'TU\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42) # generate new key based on the seed \"42\"\n",
    "params = init_params(key)\n",
    "\n",
    "x, y = X[:25], X[1:26]\n",
    "h = np.zeros(h_size)\n",
    "yhat, h = feed_forward(params, x, h)\n",
    "print(onehot_decode(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation\n",
    "\n",
    "Back propagation works, as before, using the chain rule. \n",
    "It is similar to the [FFN example](FFN.ipynb), except that the $h$ layer adds a bit of complexity, but not much.\n",
    "\n",
    "The details of the gradient calculation can be found in Stanford's [\"Convolutional Neural Networks for Visual Recognition\" course](http://cs231n.github.io/neural-networks-case-study/#grad).\n",
    "\n",
    "What's important to discuss is that instead of back propagating a single step of the network $t$, we back propagate over a sequence of steps, that is over $x=(x_1, \\ldots, x_k)$ for some arbitrary $k$.\n",
    "\n",
    "![rolled RNN](img/rolled_rnn.png)\n",
    "\n",
    "How? By \"unrolling\" the network.\n",
    "\n",
    "![Unrolled RNN](img/unrolled_rnn.png)\n",
    "\n",
    "For example, for $k=3$, the input is $x=[x(1), x(2), x(3)]$, and we can write\n",
    "\n",
    "$$\n",
    "h(1) = \\tanh{\\big(x(1) \\cdot W_x^h + h(0) \\cdot W_h^h + b_h\\big)} $$$$\n",
    "\\widehat y(1) = softmax\\big(h(1) \\cdot W_h^y\\big) $$$$\n",
    "h(2) = \\tanh{\\big(x(2) \\cdot W_x^h + h(1) \\cdot W_h^h + b_h\\big)} $$$$\n",
    "\\widehat y(2) = softmax\\big(h(2) \\cdot W_h^y\\big) $$$$\n",
    "h(3) = \\tanh{\\big(x(3) \\cdot W_x^h + h(2) \\cdot W_h^h + b_h\\big)} $$$$\n",
    "\\widehat y(3) = softmax\\big(h(3) \\cdot W_h^y\\big)\n",
    "$$\n",
    "\n",
    "The cross entropy is computed by summing over all $\\widehat y(t)$ together, and then the gradient is computed for this cross entropy with respect to the various $W$ and $b$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.17685\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(params, x, y, h):\n",
    "    yhat, h = feed_forward(params, x, h)    \n",
    "    loss = -(y * np.log(yhat)).sum()\n",
    "    return loss, h\n",
    "\n",
    "loss, h = cross_entropy(params, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation with JAX\n",
    "Now instead of manually deriving the gradient and implementing it as a Python program, we use JAX's automatic differentiation. [`jax.grad`](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html#jax-first-transformation-grad) takes a function `f(a, b, c)` and returns a function `dfda(a, b, c)` that returns the gradient of `f` with respect to `a` at the values of `a`, `b`, and `c`. It does so by automating the procedure we did manually using the chain rule.\n",
    "\n",
    "In our case, `f` is `cross_entropy`, `a` is `params`, and `b` and `c` are `x` and `y`.\n",
    "\n",
    "The function [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html#value-and-grad) is used to return both `f(a,b,c)` (the \"value\") and the `dfda` (the \"grad\"). \n",
    "Finally, `has_aux` means that `f` return two values - the value that needs to be differentiated, and an auxillary value. In our case, the value to differentiate is `loss` and the auxillary is `h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_propagation = jax.value_and_grad(cross_entropy, has_aux=True)\n",
    "\n",
    "(loss, h), grads = back_propagation(params, x, y, h)\n",
    "for p, g in zip(params, grads):\n",
    "    assert p.shape == g.shape\n",
    "    assert not (g == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer with Optax\n",
    "\n",
    "We can use a JAX implementation of the Adam optimizer from the [Optax](https://optax.readthedocs.io/) library.\n",
    "We first create the optimizer and initialize its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.001) # 0.001 is the default from Kingma et al 2014\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the optimizer to compute the updates, and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, h), grads = back_propagation(params, x, y, h)\n",
    "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JITing the training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that does all this, and pass it to `jax.jit`, which [just-in-time compiles the function](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) so it can be executed efficiently in XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit # decreases runtime from 380 ms to <1 ms!\n",
    "def update_params(params, opt_state, x, y, h):\n",
    "    (loss, h), grads = back_propagation(params, x, y, h)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, h, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625 µs ± 192 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit update_params(params, opt_state, x, y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.09128\n",
      "103.0037\n"
     ]
    }
   ],
   "source": [
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)\n",
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the network\n",
    "\n",
    "Finally, instead of a `predict` function, we have a `sample` function, which, given the parameters $W$s and $b$s and number of chars, produces a sample of text from the network.\n",
    "\n",
    "It does so by using drawing a random seed for $x_0$ and drawing $x_t$ for $t>0$ from the distribution given by $\\widehat y_t$.\n",
    "\n",
    "![](img/sampling_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nBNUBF?,qHuQvQ;EhC:?eCsz;VT,PJgML!\n",
      "ap'HoKSm:AuCiPIyNDqolZxLle';W\n",
      "UbHAdpxQMhemxPeJUCDKMGWbhiKmg!a-nyw\n"
     ]
    }
   ],
   "source": [
    "def sample(params, num_samples, key):\n",
    "    h = np.zeros(h_size)\n",
    "    \n",
    "    x = np.zeros((num_samples, vocab_size), dtype=float)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    seed_char = jax.random.choice(subkey, vocab_size)\n",
    "    x = x.at[0, seed_char].set(1)\n",
    "    \n",
    "    for t in range(1, num_samples):\n",
    "        yhat, h = step(params, x[t-1], h)\n",
    "        # draw from output distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        i = jax.random.choice(subkey, vocab_size, p=yhat)\n",
    "        x = x.at[t, i].set(1)\n",
    "    return onehot_decode(x)\n",
    "\n",
    "print(sample(params, 100, jax.random.PRNGKey(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "max_batches = 100000\n",
    "h = np.zeros(h_size)\n",
    "pos = 0\n",
    "batch = 0 \n",
    "key = jax.random.PRNGKey(8)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "params = init_params(subkey)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.001) # you can try with 0.01\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, loss 103.167320, pos 25\n",
      "\n",
      ":EsndaVZoNN\n",
      "zBRKbrVCu\n",
      "?PczTf-ASMCzQz;yRyla:.NBEoA.Ik:SdxHGQDZfnCizrRwSusRUPMUNH\n",
      "-jBCiYTL :O?inhcosCkWDpU;FUBPuooveSbu- vQKVq:j,'oIJUpnBfrEexlPNcqyOfRpDHQ bVoaZLdiCnPfvSC.hMlGZnWvtM-ZefyuFI!c,D?bL:hUVh\n",
      "--------------------------------------------------------------------------------\n",
      "batch 10000, loss 55.802208, pos 50075\n",
      "\n",
      "did bearur to my yot be ant hear.\n",
      "\n",
      "ENTRAGL:\n",
      "Sar,\n",
      "I heve far axlobsupreve seven't gion arr.\n",
      "\n",
      "PUSIR:\n",
      "Ch, dave, this tey,\n",
      "Thiuet, I walls. Tould wall. \n",
      "FyOSDETAL:\n",
      "So your laddind\n",
      "Twem dith thes pently an\n",
      "--------------------------------------------------------------------------------\n",
      "batch 20000, loss 46.399208, pos 150\n",
      "\n",
      "QUUCES IFHDLIMAS:\n",
      "Heat net you there whele as the als hes are, why have, Heirenveng: he hould he dateage fore deen of our stay dot,\n",
      "Than se dey alang for to the suntly corske'ns of huse insuld leak:\n",
      "B\n",
      "--------------------------------------------------------------------------------\n",
      "batch 30000, loss 39.962032, pos 50200\n",
      "\n",
      "Mysuld: blay roud a lost then tor own . hath be.\n",
      "\n",
      "Clafd:\n",
      "How.\n",
      "\n",
      "VALENO:\n",
      "If to my couns,\n",
      "Them.\n",
      "\n",
      "BORANTOS:\n",
      "Murebent our cedy'd,\n",
      "Tamperth he deer, I dath sacind; as Hentleen thein me,\n",
      "I am Cale thind dech\n",
      "--------------------------------------------------------------------------------\n",
      "batch 40000, loss 59.359230, pos 275\n",
      "\n",
      ";\n",
      "To maky! great the Hatnor; thy,\n",
      "Tratien: you is I was a!\n",
      "Your prreased wis diut;\n",
      "Ondinety ur.\n",
      "Thy fentlas and be thoughs?\n",
      "And, Ine siuble anl a dreat ill:\n",
      "And shalus,---atfer, and faur and cithed\n",
      "An\n",
      "--------------------------------------------------------------------------------\n",
      "batch 50000, loss 39.785316, pos 50325\n",
      "\n",
      "Y Bust be, lirt! A sair bese, you,\n",
      "To mear gnouncreant, the colle cenve go to good tome up.\n",
      "\n",
      "Silvad duspice.\n",
      "\n",
      "SIA IORO:\n",
      "My conmiors.\n",
      "\n",
      "MORK ANTONY:\n",
      "To well and\n",
      "To deem dis fall'd with my some's,\n",
      "Thone \n",
      "--------------------------------------------------------------------------------\n",
      "batch 60000, loss 37.472610, pos 400\n",
      "\n",
      "?\n",
      "\n",
      "LAdCUTHAND MINUS:\n",
      "Thyster he avine I dre for undare.\n",
      "\n",
      "DUKE OF ANRUSIO:\n",
      "Herd then bast!\n",
      "\n",
      "BESTIWAR:\n",
      "Yound staditays and fle,--o; Lord But done!\n",
      "\n",
      "Ale trou brobok my, by his infridess pasty?\n",
      "\n",
      "CKENTRAND\n",
      "--------------------------------------------------------------------------------\n",
      "batch 70000, loss 45.522884, pos 50450\n",
      "\n",
      "By RALENIUT:\n",
      "I priris:\n",
      "I lea too tell all me it my turnge ath'd gnore,\n",
      "But me.\n",
      "\n",
      "FALSTAFF:\n",
      "Tho;:\n",
      "Myshasige oll at good:\n",
      "But a his, that ever steen herry eabs none nages wet, I cawed flintly to'd gordon\n",
      "--------------------------------------------------------------------------------\n",
      "batch 80000, loss 43.261921, pos 525\n",
      "\n",
      "Cir, houselved.\n",
      "Giss fase the wist priped they:\n",
      "I sugh gert him full, for unom head blord,\n",
      "But hones in seven, my head Merour sotey\n",
      "And fith the pattere doth mumbertay, thyor fut this destresed prided\n",
      "--------------------------------------------------------------------------------\n",
      "batch 90000, loss 42.367653, pos 50575\n",
      "\n",
      ":\n",
      "Their myself dee\n",
      "'Thus be out theR I move I lugk my father wemeran's herd. Toray, you, I plar do men to my lord, as sppain donn\n",
      "An be coadin, have that wear in lord.\n",
      "\n",
      "MALVELE:\n",
      "Se cut a will bell at \n",
      "--------------------------------------------------------------------------------\n",
      "batch 100000, loss 34.426102, pos 650\n",
      "\n",
      "Re then.\n",
      "Biths hesem\n",
      "Inter Who aver!\n",
      "And and stafur me must,\n",
      "Iw head in in hith kninty, our contabs\n",
      "Thee praith! the movers, he this true my his givet, scirmble shouth speak,\n",
      "While his eaved.\n",
      "\n",
      "CLEONNR\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "while batch <= max_batches:\n",
    "    if pos + seq_length + 1 >= data_size:\n",
    "        # reset data position and hidden state\n",
    "        pos, h = 0, np.zeros(h_size) \n",
    "        \n",
    "    x = X[pos : pos + seq_length]\n",
    "    y = X[pos + 1 : pos + seq_length + 1]\n",
    "    pos += seq_length\n",
    "        \n",
    "    params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "    \n",
    "    if batch % (max_batches // 10) == 0:\n",
    "        print('batch {:d}, loss {:.6f}, pos {}'.format(batch, loss, pos))\n",
    "        print()\n",
    "        \n",
    "        key, subkey = jax.random.split(key)        \n",
    "        sample_text = sample(params, 200, subkey)\n",
    "        print(sample_text)\n",
    "        print('-'*80)\n",
    "    batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blogpost\n",
    "- [Obama-RNN](https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0) by samim.\n",
    "- [Making a Predictive Keyboard using Recurrent Neural Networks](http://curiousily.com/data-science/2017/05/23/tensorflow-for-hackers-part-5.html) by Venelin Valkov\n",
    "- [JAX tutorial](https://colinraffel.com/blog/you-don-t-know-jax.html) by Colin Raffel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com) and is part of the [_Data Science with Python_](https://github.com/yoavram/DataSciPy) workshop.\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
