{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](../logo.png)\n",
    "\n",
    "# Transformers: character-level language model\n",
    "## Yoav Ram\n",
    "\n",
    "We will see here the [**Transformer** architecture](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)).\n",
    "Transformers are the basis of large language models like OpenAI's [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)--the \"T\" stands for \"Transformer\".\n",
    "\n",
    "Here, we apply transformers to the same problem we applied RNN and GRU: text generation by pretraining a character level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax 0.4.35 cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax \n",
    "import jax.numpy as np\n",
    "print('jax', jax.__version__, jax.default_backend())\n",
    "import optax # pip install optax\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data Shakespear's writing as a text.\n",
    "The characters are converted to integers and then one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 99993\n",
      "Number of unique characters: 62\n",
      "Number of lines: 3298\n",
      "Number of words: 15893\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "That, poor contempt, or claim'd thou slept so faithful,\n",
      "I may contrive our father; and, in their defeated queen,\n",
      "Her flesh broke me and puttance of expedition house,\n",
      "And in that same that ever I lament this stomach,\n",
      "And he, nor Butly and my fury, knowing everything\n",
      "Grew daily ever, his great strength and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, that it may not pray their patience.'\n",
      "\n",
      "KING LEAR:\n",
      "The instant common maid, as we may less be\n",
      "a brave gentleman and joiner: he that finds u\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/shakespear.txt'\n",
    "with open(filename, 'rt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating \n",
    "- a list `chars` of the unique characters\n",
    "- `data_size` the number of total characters\n",
    "- `vocab_size` the number of unique characters\n",
    "- `int_to_char` a dictionary from index to char\n",
    "- `char_to_int` a dictionary from char to index\n",
    "and then we convert `data` from a string to a NumPy array of integers representing the chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "\n",
    "# char to int and vice versa\n",
    "int_to_char = dict(enumerate(chars)) #  == { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_int = dict(zip(int_to_char.values(), int_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "\n",
    "def onehot_encode(text):\n",
    "    ints = [char_to_int[c] for c in text]\n",
    "    ints = np.array(ints, dtype=int)\n",
    "    return jax.nn.one_hot(ints, vocab_size)\n",
    "\n",
    "def onehot_decode(data):\n",
    "    ints = data.argmax(axis=1).tolist()\n",
    "    chars = (int_to_char[k] for k in ints)\n",
    "    return str.join('', chars)\n",
    "\n",
    "X = onehot_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model\n",
    "## Self-attention\n",
    "\n",
    "Previous recurrent and convolution models that we used had limited access to the input. \n",
    "Convolutions could only use nearby elements in the input, and recurrent models had to keep a \"memory\" or previous elements in the input.\n",
    "\n",
    "In self-attention, this is solved by computing attention weights for each position in the sequence.\n",
    "First, we compute the **query** $q$ and **key** $k$.\n",
    "$$\n",
    "q_i = W^q x_i $$$$\n",
    "k_j = W^k x_j $$\n",
    "\n",
    "Then, the **attention weight** of element $x_i$ to element $x_j$ is determined by \n",
    "$$\n",
    "w_{ij} = softmax\\left(q_i k_j\\right)\n",
    "$$\n",
    "\n",
    "We then compute the **value** of position $j$,\n",
    "$$\n",
    "v_j = W^v x_j\n",
    "$$\n",
    "and so the **context** of position $i$, $z_i$, is given by\n",
    "$$\n",
    "z_i = \\sum_j{w_{ij} v_j}\n",
    "$$\n",
    "The context vector $z$ is then used to compute the **self-attention output**,\n",
    "$$\n",
    "\\textit{sa} = W^o z\n",
    "$$\n",
    "The learnable parameters of self-attention are the matrices $W^q$, $W^k$, $W^v$, and $W^o$.\n",
    "\n",
    "Another element is the *mask*: we want position $i$ to attend to position $j \\le i$ to preserve _causaility_ (just as in RNN, character $i$ depends on $j<i$ but not $j>i$). We implement it using a lower triangular matrix with `np.tril`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(x, W_q, W_k, W_v, W_o):\n",
    "    Q = x @ W_q # query q\n",
    "    K = x @ W_k # key k\n",
    "    V = x @ W_v # value v\n",
    "    d_k = Q.shape[-1] \n",
    "    w_logits = Q @ np.swapaxes(K, -1, -2) / np.sqrt(d_k) # logits of attention w\n",
    "    # causal mask\n",
    "    mask = np.tril(np.ones_like(w_logits)) \n",
    "    w_logits = w_logits - 1e10 * (1 - mask)\n",
    "    w = jax.nn.softmax(w_logits, axis=-1) # attention weights\n",
    "    z = w @ V # context vector - no parameters here!\n",
    "    sa = z @ W_o # self-attention output\n",
    "    return sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "We also implement positional encoding, which provides the model with information about the order of tokens without learning position embeddings.\n",
    "\n",
    "Positional encoding here is a fixed, sinusoidal function that assigns each token a unique vector based on its position in the sequence. For a given sequence length and model dimension, it:\n",
    "- Creates a grid where each row corresponds to a sequence position and each column to a model dimension.\n",
    "- Computes an \"angle\" for each position-dimension pair using a frequency term.\n",
    "- Multiplies the position index by these rates, then applies sin to even-indexed dimensions and cos to odd-indexed ones.\n",
    "- Concatenates the sine and cosine results to form a `(seq_len, d_model)` encoding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, None]\n",
    "    i = np.arange(d_model)[None, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    # apply sin to even indices in the array; cos to odd indices\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward\n",
    "\n",
    "Self-attention works on an embedding of the characters to a Euclidean space, rather than on one-hot encoded vectors, so we start by converting one-hot encoded input to integers.\n",
    "The embedding allows the self-attention mechanism to operate on continuous representations rather than sparse one-hot vectors. The embedding is implemented using a learned matrix. \n",
    "\n",
    "We then:\n",
    "- add the embedding to the positional encoding\n",
    "- compute self-attention \n",
    "- add a residual connection\n",
    "- pass the result through a shallow feed forward network\n",
    "- add a residual connection\n",
    "- give the result to a softmax model to predict the next character\n",
    "\n",
    "Note that the self-attention layer and the feed-forward layer take their input after passing it through **layer normalization**, stabilizes training by normalizing activations across the feature dimension, ensuring they have consistent mean and variance. This reduces internal covariate shift, facilitates gradient flow, and helps in training deeper networks. It’s particularly useful in transformers to improve convergence and overall performance.\n",
    "\n",
    "We also return the logits rather than the probabilities (so don't apply softmax) to improve numerical stability (avoid exponentiating since the loss function will take the log again anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(params, x, pos_enc):\n",
    "    # convert from one-hot to integers\n",
    "    x = x.argmax(axis=1)\n",
    "    # embedding lookup and add positional encoding\n",
    "    x = params['embedding'][x, :]\n",
    "    x = x + pos_enc \n",
    "    # self-attention\n",
    "    sa = self_attention(layer_norm(x), params['W_q'], params['W_k'], params['W_v'], params['W_o'])\n",
    "    # residual connection\n",
    "    x = x + sa  \n",
    "    # feed forward network     \n",
    "    hidden = jax.nn.relu(x @ params['W1'] + params['b1'])\n",
    "    ff = hidden @ params['W2'] + params['b2']\n",
    "    # residual connection\n",
    "    x = x + ff      \n",
    "    # output prediction - return logits rather than probabilities\n",
    "    logits = x @ params['W_out'] + params['b_out']\n",
    "    return logits\n",
    "\n",
    "def layer_norm(x, eps=1e-6):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    var = np.mean((x - mean) ** 2, axis=-1, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(var + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the parameters by drawing from normal distributions, mostly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "def init_params(key, seq_len, vocab_size, d_model, d_ff):\n",
    "    keys = jax.random.split(key, 8)\n",
    "    params = {\n",
    "        'embedding': jax.random.normal(keys[0], (vocab_size, d_model)) * 0.01,\n",
    "        'W_q': jax.random.normal(keys[1], (d_model, d_model)) * 0.01,\n",
    "        'W_k': jax.random.normal(keys[2], (d_model, d_model)) * 0.01,\n",
    "        'W_v': jax.random.normal(keys[3], (d_model, d_model)) * 0.01,\n",
    "        'W_o': jax.random.normal(keys[4], (d_model, d_model)) * 0.01,\n",
    "        'W1': jax.random.normal(keys[5], (d_model, d_ff)) * 0.01,\n",
    "        'b1': np.zeros((d_ff,)),\n",
    "        'W2': jax.random.normal(keys[6], (d_ff, d_model)) * 0.01,\n",
    "        'b2': np.zeros((d_model,)),\n",
    "        'W_out': jax.random.normal(keys[7], (d_model, vocab_size)) * 0.01,\n",
    "        'b_out': np.zeros((vocab_size,)),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "key = jax.random.key(0)\n",
    "seq_len = 25        # attendt to 25 characters\n",
    "d_model = 128\n",
    "d_ff = 512\n",
    "\n",
    "params = init_params(key, seq_len, vocab_size, d_model, d_ff)\n",
    "pos_enc = positional_encoding(seq_len, d_model)\n",
    "x = X[:seq_len, :]\n",
    "logits = transformer_model(params, x, pos_enc)\n",
    "assert logits.shape == (seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The loss function is a straightforward categorical cross-entropy.\n",
    "The only trick is we use `log_softmax` rather than `softmax` so we don't need to take the `log` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.21503\n"
     ]
    }
   ],
   "source": [
    "def NLL(params, x, y, pos_enc):\n",
    "    logits = transformer_model(params, x, pos_enc)\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    return -np.sum(y * log_probs)\n",
    "\n",
    "x = X[:seq_len, :]\n",
    "y = X[1:seq_len+1, :] \n",
    "loss = NLL(params, x, y, pos_enc)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "Now instead of manually deriving the gradient and implementing it as a Python program, we use JAX's automatic differentiation. [`jax.grad`](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html#jax-first-transformation-grad) takes a function `f(a, b, c)` and returns a function `dfda(a, b, c)` that returns the gradient of `f` with respect to `a` at the values of `a`, `b`, and `c`. It does so by automating the procedure we did manually using the chain rule.\n",
    "\n",
    "In our case, `f` is `NLL`, `a` is `params`, and `b` and `c` are `x` and `y`, that is, we use `grad` on `NLL(params, x, y)` to get `backprop(params, x, y)`.\n",
    "\n",
    "The function [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html#value-and-grad) is used to return both `f(a,b,c)` (the \"value\") and the `dfda` (the \"grad\"). \n",
    "Finally, `has_aux` means that `f` return two values - the value that needs to be differentiated, and an auxillary value. In our case, the value to differentiate is `loss` and the auxillary is `h`. This is important because we need to keep track of `h` and `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop = jax.value_and_grad(NLL)\n",
    "\n",
    "loss, grads = backprop(params, x, y, pos_enc)\n",
    "for k in params:\n",
    "    assert params[k].shape == grads[k].shape\n",
    "    assert not (grads[k] == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer\n",
    "\n",
    "We use a JAX implementation of the Adam optimizer from the [Optax](https://optax.readthedocs.io/) library.\n",
    "We first create the optimizer and initialize its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.001) # 0.001 is the default from Kingma et al 2014\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the optimizer to compute the updates, and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grads = backprop(params, x, y, pos_enc)\n",
    "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "params = optax.apply_updates(params, updates) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JITing the training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that does all this, and pass it to `jax.jit`, which [just-in-time compiles the function](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) so it can be executed efficiently in XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit # decreases runtime from 380 ms to <1 ms!\n",
    "def update_params(params, opt_state, x, y, pos_enc):\n",
    "    loss, grads = backprop(params, x, y, pos_enc)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730 μs ± 1.11 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit update_params(params, opt_state, x, y, pos_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.29582\n",
      "97.1355\n"
     ]
    }
   ],
   "source": [
    "params, opt_state, loss = update_params(params, opt_state, x, y, pos_enc)\n",
    "print(loss)\n",
    "params, opt_state, loss = update_params(params, opt_state, x, y, pos_enc)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the network\n",
    "\n",
    "Finally, instead of a `predict` function, we have a `sample` function, which, given the parameters and the number of samples we want, produces a sample of text from the network.\n",
    "\n",
    "It does so by drawing a random seed for $x_0$ and drawing $x_t$ for $t>0$ from the distribution given by $\\widehat y_t$, which is the softmax of the transformer output.\n",
    "\n",
    "Note that this function is computationally heavy as it iterates over each character position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h \n",
      "   iu er nnttfan h h    e niLtei pQerfe\n",
      "uyvq neqmr xeasfan f fnd ahfempe nlocmsYitghPib   -s?ooeH\n"
     ]
    }
   ],
   "source": [
    "def sample(params, num_samples, key):    \n",
    "    x = np.zeros((num_samples, vocab_size), dtype=float)\n",
    "    pos_enc = positional_encoding(num_samples, d_model)\n",
    "    keys = jax.random.split(key, num_samples)\n",
    "    seed_char = jax.random.choice(keys[0], vocab_size)\n",
    "    x = x.at[0, seed_char].set(1)\n",
    "    for t in range(1, num_samples):\n",
    "        logits = transformer_model(params, x[:t], pos_enc[:t])[t-1]\n",
    "        yhat = jax.nn.softmax(logits)\n",
    "        # draw from output distribution        \n",
    "        i = jax.random.choice(keys[t], vocab_size, p=yhat)\n",
    "        x = x.at[t, i].set(1)\n",
    "    return onehot_decode(x)\n",
    "\n",
    "print(sample(params, 100, jax.random.key(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We setup the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batches = 10000000\n",
    "pos = 0\n",
    "batch = 0 \n",
    "losses = []\n",
    "key = jax.random.key(86)\n",
    "\n",
    "seq_len = 25 # unrolling for 25 characters\n",
    "d_model = 128\n",
    "d_ff = 512\n",
    "\n",
    "params = init_params(key, seq_len, vocab_size, d_model, d_ff)\n",
    "pos_enc = positional_encoding(seq_len, d_model)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.001) # you can try with 0.01\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, loss 102.919846, pos 25\n",
      "\n",
      "lfbGIPBjXMkmYXJbLZZexlTj-KFdsI; xu'D,H,R;GAp,Nd,wg\n",
      "--------------------------------------------------------------------------------\n",
      "batch 1000000, loss 52.248795, pos 6275\n",
      "\n",
      "n I mot, begh swommu;\n",
      "And kid usoweranond merustow\n",
      "--------------------------------------------------------------------------------\n",
      "batch 2000000, loss 54.052490, pos 12525\n",
      "\n",
      "k menses speed, paremet he rutheasoukiswevoukerali\n",
      "--------------------------------------------------------------------------------\n",
      "batch 3000000, loss 44.485901, pos 18775\n",
      "\n",
      "lood wilt:\n",
      "I weept, ing inde ba gh, in is wr tleso\n",
      "--------------------------------------------------------------------------------\n",
      "batch 4000000, loss 44.551559, pos 25025\n",
      "\n",
      "QUEETH:\n",
      "Madd,\n",
      "Thavencearror helay owin.\n",
      "T,\n",
      "Th rrif\n",
      "--------------------------------------------------------------------------------\n",
      "batch 5000000, loss 50.127365, pos 31275\n",
      "\n",
      "His thip\n",
      "Welf did\n",
      "Rome my t meaknoat, mystrourasee\n",
      "--------------------------------------------------------------------------------\n",
      "batch 6000000, loss 57.003265, pos 37525\n",
      "\n",
      "J   knoust he manct,\n",
      "This H t ces cus lis flan t C\n",
      "--------------------------------------------------------------------------------\n",
      "batch 7000000, loss 50.891785, pos 43775\n",
      "\n",
      "w-boy\n",
      "\n",
      "POLINK:\n",
      "And\n",
      "BIAs lis; my, Thrustre wre ce; \n",
      "--------------------------------------------------------------------------------\n",
      "batch 8000000, loss 54.957642, pos 50025\n",
      "\n",
      "Firead car.\n",
      "\n",
      "KINSCANGARIOLUCANGESICRESALRICHN: AUG\n",
      "--------------------------------------------------------------------------------\n",
      "batch 9000000, loss 50.708733, pos 56275\n",
      "\n",
      "ll estet dielivingoods therou werwuleroLa mat bhal\n",
      "--------------------------------------------------------------------------------\n",
      "batch 10000000, loss 55.534870, pos 62525\n",
      "\n",
      "Whalle am a a my thence I d\n",
      "SAn' ngal mp wr:\n",
      "mamev\n",
      "--------------------------------------------------------------------------------\n",
      "CPU times: user 5h 20min 32s, sys: 36min 42s, total: 5h 57min 15s\n",
      "Wall time: 2h 12min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while batch <= max_batches:\n",
    "    if pos + seq_len + 1 >= data_size:\n",
    "        # reset data position and hidden state\n",
    "        pos = 0\n",
    "        \n",
    "    x = X[pos : pos + seq_len]\n",
    "    y = X[pos + 1 : pos + seq_len + 1]\n",
    "    pos += seq_len\n",
    "    \n",
    "    params, opt_state, loss = update_params(params, opt_state, x, y, pos_enc)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if batch % (max_batches // 10) == 0:\n",
    "        print('batch {:d}, loss {:.6f}, pos {}'.format(batch, loss, pos))\n",
    "        print()\n",
    "        \n",
    "        key, subkey = jax.random.split(key)        \n",
    "        sample_text = sample(params, 50, subkey)\n",
    "        print(sample_text)\n",
    "        print('-'*80)\n",
    "    batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Vaswani et al. 2017](http://arxiv.org/abs/1706.03762): _Attention Is All You Need_, the fundamental paper on transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com).\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
