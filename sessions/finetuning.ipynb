{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "#VSC-2a6581f1",
        "language": "markdown"
      },
      "source": [
        "![Py4Eng](../logo.png)\n",
        "\n",
        "# Finetuning pretrained model\n",
        "## Yoav Ram\n",
        "\n",
        "This notebook shows a simple, step-by-step finetuning workflow using a pretrained [EfficientNetV2](https://arxiv.org/pdf/2104.00298) backbone (from `timm`) and PyTorch. The target dataset is the [Hyena ID 2022](https://lila.science/datasets/hyena-id-2022/) dataset (3104 photos, 256 individuals) — the task is per-individual classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "We will load the pretrained model using [`timm`](https://timm.fast.ai), a collection of state-of-the-art computer vision models. Install it with `pip install timm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch 2.7.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "import timm\n",
        "\n",
        "print('torch', torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data\n",
        "\n",
        "We Download the [Hyena ID 2022 dataset](https://lila.science/datasets/hyena-id-2022/) and extract to `data/hyena`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/wild-me/hyena.coco.tar.gz\"\n",
        "out_path = '../data/hyena.coco.tar.gz'\n",
        "extract_dir = '../data/hyena'\n",
        "chunk = 1024 * 1024\n",
        "\n",
        "os.makedirs(os.path.dirname(out_path), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction directory already exists, skipping extraction.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(extract_dir):\n",
        "    if not os.path.exists(out_path):\n",
        "        print(f'Downloading {url} to {out_path}')\n",
        "        with urllib.request.urlopen(url) as r, open(out_path, 'wb') as f:\n",
        "            total = r.getheader('Content-Length')\n",
        "            total = int(total) if total else None\n",
        "            with tqdm(total=total, unit='B', unit_scale=True, desc='download') as p:\n",
        "                while True:\n",
        "                    data = r.read(chunk)\n",
        "                    if not data:\n",
        "                        break\n",
        "                    f.write(data)\n",
        "                    p.update(len(data))\n",
        "    else:\n",
        "        print(f'File {out_path} already exists, skipping download.')\n",
        "\n",
        "    print(f\"Extracting {out_path}\")\n",
        "    with tarfile.open(out_path, 'r:gz') as t:\n",
        "        os.makedirs(extract_dir, exist_ok=True)\n",
        "        t.extractall(path=extract_dir)\n",
        "else:\n",
        "    print(f'Extraction directory already exists, skipping extraction.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the hyena images are now in `../data/hyena/hyena.coco/images/train2022` folder (`test2022` and `val2022` are empty). Image filenames are just the image running number.\n",
        "\n",
        "The metadata is in `../data/hyena/hyena.coco/annotations/instances_train2022.json`, which contains the bounding box of the individuals in the images, as well as their the identities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "IMG_DIR = Path('../data/hyena/hyena.coco/images/train2022')\n",
        "ANNO = Path('../data/hyena/hyena.coco/annotations/instances_train2022.json')\n",
        "if not IMG_DIR.exists() or not ANNO.exists():\n",
        "    print('Images or annotations not found:')\n",
        "    print('IMG_DIR =', IMG_DIR.exists(), IMG_DIR)\n",
        "    print('ANNO =', ANNO.exists(), ANNO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load COCO-style annotations and build mapping from filename to identity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(ANNO, 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "img2name = {a['image_id']: a['name'] for a in metadata['annotations']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build label->index mapping\n",
        "unique_labels = sorted({lab for _, lab in samples})\n",
        "label2idx = {lab: i for i, lab in enumerate(unique_labels)}\n",
        "NUM_CLASSES = len(unique_labels)\n",
        "\n",
        "# Convert samples to (path, idx)\n",
        "samples = [(p, label2idx[l]) for p, l in samples]\n",
        "\n",
        "# Shuffle and split 90/10\n",
        "random.seed(42)\n",
        "random.shuffle(samples)\n",
        "cut = int(len(samples) * 0.9)\n",
        "train_samples = samples[:cut]\n",
        "val_samples = samples[cut:]\n",
        "\n",
        "print(f'Total images with annotation: {len(samples)}, classes: {NUM_CLASSES}')\n",
        "print('Train / Val sizes =', len(train_samples), len(val_samples))\n",
        "\n",
        "# Simple Dataset that reads images from path\n",
        "\n",
        "\n",
        "class HyenaDataset(Dataset):\n",
        "    def __init__(self, samples, transform=None):\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p, label = self.samples[idx]\n",
        "        img = Image.open(p).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definte the image transforms: resize to the image size suitable for the model, convert to PyTorch tensor, and normalize colors. For the training transforms, we also augment the data using horizontal flips."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "\n",
        "val_transforms = T.Compose([    \n",
        "    T.CenterCrop(IMG_SIZE),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_transforms = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the datasets and data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataLoader params\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 224\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "train_ds = HyenaDataset(train_samples, transform=train_transforms)\n",
        "val_ds = HyenaDataset(val_samples, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "print('DataLoaders ready.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Model setup\n",
        "Load a pretrained EfficientNetV2 from `timm`, replace the classifier head with a new linear layer sized to `NUM_CLASSES`. We keep the code defensive to handle a couple of naming conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some sample images and their labels (visual check)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if 'train_ds' in globals():\n",
        "    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    axs = axs.ravel()\n",
        "    for i in range(10):\n",
        "        img, label = train_ds[i]\n",
        "        # img is normalized tensor; un-normalize for display\n",
        "        img_np = img.numpy().transpose(1, 2, 0)\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img_np = img_np * std + mean\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "        axs[i].imshow(img_np)\n",
        "        axs[i].set_title(f'label={label}')\n",
        "        axs[i].axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print('train_ds not found. Run Data loaders cell first.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "# Create model (small variant for speed; change to efficientnetv2_m or _l if you want)\n",
        "MODEL_NAME = 'efficientnetv2_s'\n",
        "\n",
        "# lazy guard: NUM_CLASSES must exist from dataset cell\n",
        "try:\n",
        "    NUM_CLASSES\n",
        "except NameError:\n",
        "    NUM_CLASSES = 256  # fallback; change to actual number after you build split\n",
        "\n",
        "model = timm.create_model(MODEL_NAME, pretrained=True)\n",
        "# replace classifier robustly\n",
        "if hasattr(model, 'classifier') and isinstance(model.classifier, nn.Linear):\n",
        "    in_f = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_f, NUM_CLASSES)\n",
        "elif hasattr(model, 'fc') and isinstance(model.fc, nn.Linear):\n",
        "    in_f = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_f, NUM_CLASSES)\n",
        "else:\n",
        "    # timm models often have a .get_classifier / .reset_classifier API; try that\n",
        "    try:\n",
        "        model.reset_classifier(num_classes=NUM_CLASSES)\n",
        "    except Exception as e:\n",
        "        print('Could not find a known classifier attribute; inspect model to set classifier manually:', e)\n",
        "\n",
        "model.to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "### Freeze backbone and train head\n",
        "We freeze all parameters except the classifier parameters (by name heuristic) and train only the head for a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Freeze everything except classifier\n",
        "for name, p in model.named_parameters():\n",
        "    if 'classifier' in name or 'head' in name or 'fc' in name:\n",
        "        p.requires_grad = True\n",
        "    else:\n",
        "        p.requires_grad = False\n",
        "\n",
        "# Simple optimizer for the trainable params only\n",
        "head_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(head_params, lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def evaluate(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return correct / total if total else 0.0\n",
        "\n",
        "def train_one_epoch(loader, model, opt, loss_fn):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    return running_loss / (len(loader.dataset) if hasattr(loader, 'dataset') else 1)\n",
        "\n",
        "# Small sanity run (only if loaders exist)\n",
        "if 'train_loader' in globals():\n",
        "    EPOCHS_HEAD = 3\n",
        "    for ep in range(EPOCHS_HEAD):\n",
        "        loss = train_one_epoch(train_loader, model, optimizer, criterion)\n",
        "        val_acc = evaluate(val_loader, model)\n",
        "        print(f'Head epoch {ep+1}/{EPOCHS_HEAD} - loss {loss:.4f} - val_acc {val_acc:.4f}')\n",
        "else:\n",
        "    print('No dataloaders found. Create splits and DataLoaders first.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "### Unfreeze and finetune the whole model\n",
        "Now we unfreeze all parameters and continue training with a smaller learning rate. This typically improves performance but is slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Unfreeze all params\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# New optimizer with lower LR\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Train a few more epochs\n",
        "if 'train_loader' in globals():\n",
        "    EPOCHS_FULL = 3\n",
        "    for ep in range(EPOCHS_FULL):\n",
        "        loss = train_one_epoch(train_loader, model, optimizer, criterion)\n",
        "        val_acc = evaluate(val_loader, model)\n",
        "        print(f'Finetune epoch {ep+1}/{EPOCHS_FULL} - loss {loss:.4f} - val_acc {val_acc:.4f}')\n",
        "else:\n",
        "    print('No dataloaders found. Create splits and DataLoaders first.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Next steps and tips\n",
        "- Increase `IMG_SIZE`, `BATCH_SIZE`, and `EPOCHS` for higher final accuracy.\n",
        "- Use a learning rate schedule (Cosine/Step), weight decay, and label smoothing for better generalization.\n",
        "- Consider stratified splits by individual to ensure balanced train/val per-class.\n",
        "- If memory is tight, use mixed precision (`torch.cuda.amp`).\n",
        "\n",
        "That's it — the notebook is intentionally simple so you can run and understand each step."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DataSciPy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
